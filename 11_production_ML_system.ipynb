{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production ML Systems\n",
    "\n",
    "There's a lot more to machine learning than just implementing an ML algorithm. A production ML system involves a significant number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/ml-system.png)\n",
    "![](img/ml-system2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML code is at the heart of a real-world ML production system, but that box often represents only 5% or less of the overall code of that total ML production system. (That's not a misprint.) Notice that a ML production system devotes considerable resources to input data—collecting it, verifying it, and extracting features from it. Furthermore, notice that a serving infrastructure must be in place to put the ML model's predictions into practical use in the real world.\n",
    "\n",
    "Fortunately, many of the components in the preceding figure are reusable. Furthermore, you don't have to build all the components in Figure 1 yourself.\n",
    "\n",
    "TensorFlow provides many of these components, but other options are available from other platforms such as Spark or Hadoop.\n",
    "\n",
    "Subsequent modules will help guide your design decisions in building a production ML system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static vs. Dynamic Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, there are two ways to train a model:\n",
    "\n",
    "- A **static model** is trained offline. That is, we train the model exactly once and then use that trained model for a while.\n",
    "- A **dynamic model** is trained online. That is, data is continually entering the system and we're incorporating that data into the model through continuous updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, the following points dominate the static vs. dynamic training decision:\n",
    "\n",
    "- Static models are easier to build and test.\n",
    "- Dynamic models adapt to changing data. The world is a highly changeable place. Sales predictions built from last year's data are unlikely to successfully predict next year's results.\n",
    "\n",
    "If your data set truly isn't changing over time, choose static training because it is cheaper to create and maintain than dynamic training. However, many information sources really do change over time, even those with features that you think are as constant as, say, sea level. The moral: even with static training, you must still monitor your input data for change.\n",
    "\n",
    "For example, consider a model trained to predict the probability that users will buy flowers. Because of time pressure, the model is trained only once using a dataset of flower buying behavior during July and August. The model is then shipped off to serve predictions in production, but is never updated. The model works fine for several months, but then makes terrible predictions around Valentine's Day because user behavior during that holiday period changes dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static vs. Dynamic Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose either of the following inference strategies:\n",
    "\n",
    "- **offline inference**, meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table.\n",
    "- **online inference**, meaning that you predict on demand, using a server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Lecture Summary\n",
    "Here are the pros and cons of offline inference:\n",
    "\n",
    "- Pro: Don’t need to worry much about cost of inference.\n",
    "- Pro: Can likely use batch quota or some giant MapReduce.\n",
    "- Pro: Can do post-verification of predictions before pushing.\n",
    "- Con: Can only predict things we know about — bad for long tail.\n",
    "- Con: Update latency is likely measured in hours or days.\n",
    "\n",
    "Here are the pros and cons of online inference:\n",
    "\n",
    "- Pro: Can make a prediction on any new item as it comes in — great for long tail.\n",
    "- Con: Compute intensive, latency sensitive—may limit model complexity.\n",
    "- Con: Monitoring needs are more intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is as important to ML developers as code is to traditional programmers. This lesson focuses on the kinds of questions you should be asking of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of an ML system is dependent on the behavior and qualities of its input features. As the input data for those features changes, so too will your model. Sometimes that change is desirable, but sometimes it is not.\n",
    "\n",
    "In traditional software development, you focus more on code than on data. In machine learning development, although coding is still part of the job, your focus must widen to include data. For example, on traditional software development projects, it is a best practice to write unit tests to validate your code. On ML projects, you must also continuously test, verify, and monitor your input data.\n",
    "\n",
    "For example, you should continuously monitor your model to remove unused (or little used) features. Imagine a certain feature that has been contributing little or nothing to the model. If the input data for that feature abruptly changes, your model's behavior might also abruptly change in undesirable ways.\n",
    "\n",
    "### Reliability\n",
    "Some questions to ask about the reliability of your input data:\n",
    "\n",
    "- Is the signal always going to be available or is it coming from an unreliable source? For example:\n",
    "- Is the signal coming from a server that crashes under heavy load?\n",
    "- Is the signal coming from humans that go on vacation every August?\n",
    "\n",
    "### Versioning\n",
    "Some questions to ask about versioning:\n",
    "\n",
    "- Does the system that computes this data ever change? If so:\n",
    "- How often?\n",
    "- How will you know when that system changes?\n",
    "\n",
    "Sometimes, data comes from an upstream process. If that process changes abruptly, your model can suffer.\n",
    "\n",
    "Consider creating your own copy of the data you receive from the upstream process. Then, only advance to the next version of the upstream data when you are certain that it is safe to do so.\n",
    "\n",
    "### Necessity\n",
    "The following question might remind you of regularization:\n",
    "\n",
    "- Does the usefulness of the feature justify the cost of including it?\n",
    "\n",
    "It is always tempting to add more features to the model. For example, suppose you find a new feature whose addition makes your model slightly more accurate. More accuracy certainly sounds better than less accuracy. However, now you've just added to your maintenance burden. That additional feature could degrade unexpectedly, so you've got to monitor it. Think carefully before adding features that lead to minor short-term wins.\n",
    "\n",
    "### Correlations\n",
    "Some features correlate (positively or negatively) with other features. Ask yourself the following question:\n",
    "\n",
    "- Are any features so tied together that you need additional strategies to tease them apart?\n",
    "\n",
    "### Feedback Loops\n",
    "Sometimes a model can affect its own training data. For example, the results from some models, in turn, are directly or indirectly input features to that same model.\n",
    "\n",
    "Sometimes a model can affect another model. For example, consider two models for predicting stock prices:\n",
    "\n",
    "- Model A, which is a bad predictive model.\n",
    "- Model B.\n",
    "\n",
    "Since Model A is buggy, it mistakenly decides to buy stock in Stock X. Those purchases drive up the price of Stock X. Model B uses the price of Stock X as an input feature, so Model B can easily come to some false conclusions about the value of Stock X stock. Model B could, therefore, buy or sell shares of Stock X based on the buggy behavior of Model A. Model B's behavior, in turn, can affect Model A, possibly triggering a tulip mania or a slide in Company X's stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Effective ML Guidelines\n",
    "\n",
    "Here's a quick synopsis of effective ML guidelines:\n",
    "\n",
    "- Keep your first model simple.\n",
    "- Focus on ensuring data pipeline correctness.\n",
    "- Use a simple, observable metric for training & evaluation.\n",
    "- Own and monitor your input features.\n",
    "- Treat your model configuration as code: review it, check it in.\n",
    "- Write down the results of all experiments, especially \"failures.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
