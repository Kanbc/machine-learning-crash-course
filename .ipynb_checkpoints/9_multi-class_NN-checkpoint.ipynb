{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One vs. all** provides a way to leverage binary classification. Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers—one binary classifier for each possible outcome. During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question. For example, given a picture of a dog, five different recognizers might be trained, four seeing the image as a negative example (not a dog) and one seeing the image as a positive example (a dog). That is:\n",
    "\n",
    "1. Is this image an apple? No.\n",
    "2. Is this image a bear? No.\n",
    "3. Is this image candy? No.\n",
    "4. Is this image a dog? Yes.\n",
    "5. Is this image an egg? No.\n",
    "\n",
    "This approach is fairly reasonable when the total number of classes is small, but becomes increasingly inefficient as the number of classes rises.\n",
    "\n",
    "We can create a significantly more efficient one-vs.-all model with a deep neural network in which each output node represents a different class. The following figure suggests this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/multi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, returning to the image analysis we saw in Figure 1, Softmax might produce the following likelihoods of an image belonging to a particular class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax equation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y = j|\\textbf{x})  = \\frac{e^{(\\textbf{w}_j^{T}\\textbf{x} + b_j)}}{\\sum_{k\\in K} {e^{(\\textbf{w}_k^{T}\\textbf{x} + b_k)}} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that this formula basically extends the formula for logistic regression into multiple classes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Options\n",
    "Consider the following variants of Softmax:\n",
    "\n",
    "- **Full Softmax** is the Softmax we've been discussing; that is, Softmax calculates a probability for every possible class.\n",
    "\n",
    "- **Candidate sampling** means that Softmax calculates a probability for all the positive labels but only for a random sample of negative labels. For example, if we are interested in determining whether an input image is a beagle or a bloodhound, we don't have to provide probabilities for every non-doggy example.\n",
    "\n",
    "Full Softmax is fairly cheap when the number of classes is small but becomes prohibitively expensive when the number of classes climbs. Candidate sampling can improve efficiency in problems having a large number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Label vs. Many Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:\n",
    "\n",
    "- You may not use Softmax.\n",
    "- You must rely on multiple logistic regressions.\n",
    "\n",
    "For example, suppose your examples are images containing exactly one item—a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If your examples are images containing all sorts of things—bowls of different kinds of fruit—then you'll have to use multiple logistic regressions instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's download the data set, import TensorFlow and other utilities, and load the data into a *pandas* `DataFrame`. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8266</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8643</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
       "3408    6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "8266    9    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "8643    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "874     6    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "448     1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "\n",
       "      778  779  780  781  782  783  784  \n",
       "3408    0    0    0    0    0    0    0  \n",
       "8266    0    0    0    0    0    0    0  \n",
       "8643    0    0    0    0    0    0    0  \n",
       "874     0    0    0    0    0    0    0  \n",
       "448     0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "mnist_dataframe = pd.read_csv(\n",
    "  \"https://storage.googleapis.com/mledu-datasets/mnist_train_small.csv\",\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "# Use just the first 10,000 records for training/validation.\n",
    "mnist_dataframe = mnist_dataframe.head(10000)\n",
    "\n",
    "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
    "mnist_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels_and_features(dataset):\n",
    "    \"\"\"Extracts labels and features.\n",
    "\n",
    "    This is a good place to scale or transform the features if needed.\n",
    "\n",
    "    Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "    Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "    \"\"\"\n",
    "    labels = dataset[0]\n",
    "\n",
    "    # DataFrame.loc index ranges are inclusive at both ends.\n",
    "    features = dataset.loc[:,1:784]\n",
    "    # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "    features = features / 255\n",
    "\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1      2      3      4      5      6      7      8      9      10   \\\n",
       "count 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
       "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...      775    776    777    778    779    780    781    782    783  \\\n",
       "count  ...   7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
       "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max    ...      1.0    1.0    0.8    0.2    1.0    0.2    0.0    0.0    0.0   \n",
       "\n",
       "         784  \n",
       "count 7500.0  \n",
       "mean     0.0  \n",
       "std      0.0  \n",
       "min      0.0  \n",
       "25%      0.0  \n",
       "50%      0.0  \n",
       "75%      0.0  \n",
       "max      0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
    "training_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1      2      3      4      5      6      7      8      9      10   \\\n",
       "count 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
       "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...      775    776    777    778    779    780    781    782    783  \\\n",
       "count  ...   2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
       "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "max    ...      1.0    0.8    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "         784  \n",
       "count 2500.0  \n",
       "mean     0.0  \n",
       "std      0.0  \n",
       "min      0.0  \n",
       "25%      0.0  \n",
       "50%      0.0  \n",
       "75%      0.0  \n",
       "max      0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
    "validation_examples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a random example and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_example = np.random.choice(training_examples.index)\n",
    "_, ax = plt.subplots()\n",
    "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
    "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEGCAYAAACq4kOvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127848940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Build a Linear Model for MNIST\n",
    "\n",
    "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
    "\n",
    "You'll notice that in addition to reporting accuracy, and plotting Log Loss over time, we also display a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were misclassified as other classes. Which digits get confused for each other?\n",
    "\n",
    "Also note that we track the model's error using the `log_loss` function. This should not be confused with the loss function internal to `LinearClassifier` that is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "    \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "    Returns:\n",
    "    A set of feature columns\n",
    "    \"\"\" \n",
    "\n",
    "    # There are 784 pixels in each image.\n",
    "    return set([tf.feature_column.numeric_column('pixels', shape=784)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
    "    \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
    "\n",
    "    Args:\n",
    "    features: The training features.\n",
    "    labels: The training labels.\n",
    "    batch_size: Batch size to use during training.\n",
    "\n",
    "    Returns:\n",
    "    A function that returns batches of training features and labels during\n",
    "    training.\n",
    "    \"\"\"\n",
    "    def _input_fn(num_epochs=None, shuffle=True):\n",
    "        # Input pipelines are reset with each call to .train(). To ensure model\n",
    "        # gets a good sampling of data, even when number of steps is small, we \n",
    "        # shuffle all the data before creating the Dataset object\n",
    "        idx = np.random.permutation(features.index)\n",
    "        raw_features = {\"pixels\":features.reindex(idx)}\n",
    "        raw_targets = np.array(labels[idx])\n",
    "\n",
    "        ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "\n",
    "        # Return the next batch of data.\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_input_fn(features, labels, batch_size):\n",
    "    \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
    "\n",
    "    Args:\n",
    "    features: The features to base predictions on.\n",
    "    labels: The labels of the prediction examples.\n",
    "\n",
    "    Returns:\n",
    "    A function that returns features and labels for predictions.\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        raw_features = {\"pixels\": features.values}\n",
    "        raw_targets = np.array(labels)\n",
    "\n",
    "        ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
    "        ds = ds.batch(batch_size)\n",
    "\n",
    "\n",
    "        # Return the next batch of data.\n",
    "        feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "        return feature_batch, label_batch\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
    "\n",
    "    In addition to training, this function also prints training progress information,\n",
    "    a plot of the training and validation loss over time, and a confusion\n",
    "    matrix.\n",
    "\n",
    "    Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "\n",
    "    Returns:\n",
    "    The trained `LinearClassifier` object.\n",
    "    \"\"\"\n",
    "\n",
    "    periods = 10\n",
    "\n",
    "    steps_per_period = steps / periods  \n",
    "    # Create the input functions.\n",
    "    predict_training_input_fn = create_predict_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "    predict_validation_input_fn = create_predict_input_fn(\n",
    "    validation_examples, validation_targets, batch_size)\n",
    "    training_input_fn = create_training_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "\n",
    "    # Create a LinearClassifier object.\n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    classifier = tf.estimator.LinearClassifier(\n",
    "      feature_columns=construct_feature_columns(),\n",
    "      n_classes=10,\n",
    "      optimizer=my_optimizer,\n",
    "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
    "    )\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss error (on validation data):\")\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "\n",
    "        # Take a break and compute probabilities.\n",
    "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
    "\n",
    "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
    "\n",
    "        # Compute training and validation errors.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "    print(\"Model training finished.\")\n",
    "    # Remove event files to save disk space.\n",
    "    _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "\n",
    "    # Calculate final predictions (not probabilities, as above).\n",
    "    final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Output a plot of the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class).\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "    ax.set_aspect(1)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a set of parameters that should attain roughly 0.9 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss error (on validation data):\n",
      "  period 00 : 4.17\n",
      "  period 01 : 3.85\n",
      "  period 02 : 3.67\n",
      "  period 03 : 3.50\n",
      "  period 04 : 3.19\n",
      "  period 05 : 3.19\n",
      "  period 06 : 3.21\n",
      "  period 07 : 3.14\n",
      "  period 08 : 3.29\n",
      "  period 09 : 3.15\n",
      "Model training finished.\n",
      "Final accuracy (on validation data): 0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBWOMMRksKRhjjMnw/7X4ayxLvUTOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132153ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEWCAYAAAAdG+ASAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHxhJREFUeJzt3Xm4XFWZ7/Hv75yQkMmgEGggQQICGmlBhshFRRShE0Xg\n9qMtQ4PYQC62IDa3bVHoVpy9ajs0SAiTUwMOQDdiWlBvO6AMCcgUJkMQEqZEZhIkOcl7/9jrQKVu\nzqk6VXufqr3r9/HZD1V7eNc6yfHN2nuvQRGBmVlV9XW6AmZmRXKSM7NKc5Izs0pzkjOzSnOSM7NK\nc5Izs0pzkqswSeMl/VjS05J+2EacoyRdk2fdOkXSmyXd0+l62OiR+8l1nqQjgVOBVwPPArcAn42I\na9uMezRwMrBvRAy0XdEuJymAnSJiSafrYt3DLbkOk3Qq8DXgc8BWwHbA2cAhOYR/JXBvLyS4Zkga\n0+k6WAdEhLcObcAU4DngPcOcM44sCT6ctq8B49Kx/YHlwP8GVgCPAO9Px84E1gBrUxnHAZ8EvlcT\ne3sggDHp+7HAUrLW5P3AUTX7r625bl9gIfB0+u++Ncd+CXwa+G2Kcw2wxRA/22D9/6mm/ocB7wDu\nBZ4APl5z/izgOuCpdO5ZwNh07NfpZ1mVft731sT/KPAo8N3BfemaHVMZe6Tv2wArgf07/bvhLcf/\nn3W6Ar28AbOBgcEkM8Q5nwKuB7YEpgK/Az6dju2frv8UsElKDquBl6fj9UltyCQHTASeAXZJx7YG\nXps+v5jkgFcATwJHp+uOSN83T8d/CdwH7AyMT9+/MMTPNlj/f0n1PyElmYuBycBrgeeBGen8PYF9\nUrnbA3cBH66JF8CrNhL/i2T/WIyvTXLpnBOAO4EJwNXAlzv9e+Et3823q521OfCnGP528ijgUxGx\nIiJWkrXQjq45vjYdXxsRC8haMbu0WJ/1wK6SxkfEIxGxeCPnvBP4Q0R8NyIGIuIS4G7gXTXnXBQR\n90bE88APgN2HKXMt2fPHtcClwBbA1yPi2VT+ncBuABFxU0Rcn8r9I3Au8JYmfqZPRMQLqT4biIjz\ngCXADWSJ/fQG8axknOQ663FgiwbPirYBHqj5/kDa92KMuiS5Gpg00opExCqyW7wTgUck/UTSq5uo\nz2Cdtq35/ugI6vN4RKxLnweT0GM1x58fvF7SzpKukvSopGfInmNuMUxsgJUR8ecG55wH7Ar8W0S8\n0OBcKxknuc66DniB7DnUUB4me4EwaLu0rxWryG7LBv1F7cGIuDoiDiRr0dxN9n/+RvUZrNNDLdZp\nJM4hq9dOEfEy4OOAGlwzbPcBSZPInnNeAHxS0ivyqKh1Dye5DoqIp8meR50t6TBJEyRtImmOpP+T\nTrsEOEPSVElbpPO/12KRtwD7SdpO0hTgY4MHJG0l6VBJE8kS73Nkt3r1FgA7SzpS0hhJ7wVmAle1\nWKeRmEz23PC51Mr8QN3xx4AdRhjz68CiiDge+Akwr+1aWldxkuuwiPgKWR+5M8geui8DTgL+I53y\nGWARcBtwO3Bz2tdKWT8Dvp9i3cSGiakv1eNhsjeOb+H/TyJExOPAwWRvdB8nezN6cET8qZU6jdA/\nAkeSvbU9j+xnqfVJ4NuSnpL0N42CSTqU7OXP4M95KrCHpKNyq7F1nDsDm1mluSVnZpXmJGdmleYk\nZ2aV5iRnZpXWtQOWx47dNPc3ImvWNOoT2hvK9LJpfUF17e8r5t/3P69dm3vMcWOK+7+ppEb9DBtd\n3/RfUES0VVar3JIzs0rr2pacmXW/NhuCo8JJzsxa1tfX3+kqNOQkZ2ZtcEvOzCrMt6tmVmlOcmZW\naVL3d9BwkjOzlvV0Sy7N93UoL80Y+xBwZUTcVVSZZja6+grqVJ2nQmoo6aNk8/ULuDFtAi6RdFoR\nZZrZ6JPU9NaxOhYxxEfSvWQrPa2t2z8WWBwROw1x3VxgLkB//5g98+6D42FdGQ/r8rCuQe0O65o4\ncbOm/4JWrXqqUsO61rPhYiuDtmbjU2oDEBHzI2KviNirDJ0MzXpdGVpyRf0T8WHgF5L+QDadN2SL\nnbyKbGpvM6uAnn3xEBE/lbQz2YrntS8eFtYsP2dmJVeGFw+F3exHxHqyld/NrKLcT87MKq1nb1fN\nrDc4yZlZxTnJmVmF+ZmcmVVaT79dNbPq8zO5NhQxBKuov5AyDZOCYv4civozKGr4VVGKGILVzYmk\nm+s2qGuTnJl1Pz+TM7NKc0vOzCrNLTkzqzQnOTOrNN+umlmlOcmZWaU5yZlZpTnJmVml9an7lykY\n9Vcjkt4/2mWaWUGk5rcO6cT73zOHOiBprqRFkhbNnz9/NOtkZi3o2YVsJN021CFgq6Gui4j5wGB2\nK9eAULMe1MvP5LYC/gp4sm6/gN8VVKaZjbJe7gx8FTApIm6pPyDplwWVaWajrAwtuULScEQcFxHX\nDnHsyCLKNLPR19fX1/TWDEmzJd0jaYmk0zZyfIqkH0u6VdLiZl5kdn9b08y6luhremsYS+oHzgbm\nADOBIyTNrDvtg8CdEbEbsD/wFUljh4vrJGdmrcu3C8ksYElELI2INcClwKF15wQwWdl98iTgCWBg\nuKBOcmbWspF0IantIpa2uXXhtgWW1XxfnvbVOgt4DfAwcDtwSlrIfkge8WBmLRvJi4e6LmKt+ivg\nFuBtwI7AzyT9JiKeGeoCt+TMrGV9ff1Nb014CJhe831a2lfr/cDlkVkC3A+8erigXduSW7d+2BZo\nSwbWrcs9JsDkya8oJO4TT60oJK4KWBC4qAVnivg9gHItvFPkQkntdgHJuQvJQmAnSTPIktvhQH1v\njAeBA4DfSNoK2AVYOlzQrk1yZtb98kxyETEg6STgaqAfuDAiFks6MR2fB3wa+Jak28kGF3w0Iv40\nXFwnOTNrQ753BRGxAFhQt29ezeeHgYNGEtNJzsxa1svDusysB5RhWJeTnJm1rNnhWp3kJGdmLXNL\nzswqzc/kzKzS3JIzs0oromN53gpra0p6taQDJE2q2z+7qDLNbJT16kI2kj4E/CdwMnCHpNrpUj5X\nRJlmNvpyHrtaTB0LinsCsGdEHEY2sd0/SzolHRsypddOxXKeV+sy63o9u1oX0BcRzwFExB8l7Q/8\nSNIrGSbJ1U7Fsm79eq/WZdblyvDioaiW3GOSdh/8khLewcAWwF8WVKaZjbJebskdQ92UxBExABwj\n6dyCyjSzUdaz/eQiYvkwx35bRJlmNvp6NsmZWW8owzM5Jzkza5mTnJlVmm9XzazS3JIzs0pzkmtD\nXwF/eEWt/LTyiccKiTttmx0LifvYYw/kHvOp1atyjwkwedPxhcQtShG/Y2P6OzckqhGpe+s2qGuT\nnJl1P7fkzKzSnOTMrNKc5Mys0pzkzKzS3E/OzCrNSxKaWcX5dtXMKszP5Mys0nr6mZykWUBExEJJ\nM4HZwN0RsaCoMs1sdJWhJVfUal2fAL4BnCPp88BZwETgNEmnD3PdiwvZzPdCNmZdr6+vr+mtU4pq\nyb0b2B0YBzwKTIuIZyR9GbgB+OzGLqpdyCYivJCNWZfr5dvVgYhYB6yWdF9EPAMQEc9LKmaUvJmN\nup69XQXWSJqQPu85uFPSFMBJzqwyNIKtM4pqye0XES8ARERtUtsEeF9BZZrZKOvZltxggtvI/j9F\nxO1FlGlmo099anprKp40W9I9kpZIOm2Ic/aXdIukxZJ+1Sim+8mZWcvyfGuqbAbOs4EDgeXAQklX\nRsSdNedsBnwTmB0RD0rasmEdc6uhmfUcSU1vTZgFLImIpRGxBrgUOLTunCOByyPiQYCIWNEoqJOc\nmbVsJEmuth9s2ubWhdsWWFbzfXnaV2tn4OWSfinpJknHNKqjb1fNrGUj6SZX2w+2DWPIemwcAIwH\nrpN0fUTcO9wFGyXpZcOVNNj3zcx6WL5vVx8Cptd8n5b21VoOPB4Rq4BVkn4N7AaMPMkBi4Fgww4u\ng98D2K7pqregiFfTRb3uHlPQkJUiVtUCmDhxs9xjrlr1VO4xi1TUgJoifseeXr0695iDpkyY0Pik\nYeQ8XGshsJOkGWTJ7XCyZ3C1/hM4S9IYYCzwBuCrwwUdMslFxPShjpmZQb5JPSIGJJ0EXA30AxdG\nxGJJJ6bj8yLiLkk/BW4jG1hwfkTcMVzcpp7JSToc2CEiPidpGrBVRNzUzg9kZuXXbP+3ZqVZihbU\n7ZtX9/1LwJeajdmwrSnpLOCtwNFp12pg3tBXmFmvyLkLSSGaacntGxF7SPo9QEQ8IWlswfUysxIo\nw7CuZpLcWmXzqQSApM3xIHszI++Xq8VoJsmdDVwGTJV0JvA3wJmF1srMSkH93T+eoGGSi4jvSLoJ\neHva9Z5GbzPMrDdU5XYVste5a8luWbs/dZvZqChDkmvm7erpwCXANmQ9kC+W9LGRFiTpOyOvnpl1\ns6q8XT0GeH1ErAaQ9Fng98Dnh7pA0pX1u4C3pmlSiIhDWquumXWTMrTkmklyj9SdNybtG8404E7g\nfF4aCrYX8JXhLkqzEswFOPfcc5k7t36SAjPrJnl3Bi7CcAP0v0qWoJ4AFku6On0/iGyM2XD2Ak4B\nTgc+EhG3SHo+IoadxbNulgKv1mXW5frKnOSAwTeoi4Gf1Oy/vlHQtK7DVyX9MP33sQZlmVkZlfl2\nNSIuaDd4RCwH3iPpnYCnZjKrmEo8k5O0I9li0DOBTQf3R8TOzRYSET9hw9agmVVAGZ7JNdPn7VvA\nRWQvD+YAPwC+X2CdzKwkytCFpJkkNyEirgaIiPsi4gyyZGdmPa6vr6/prVOaeRnwQhqgf1+avO4h\nYHKx1TKzMijBI7mmktw/ABOBD5E9m5sC/F2RlTKzkijBM7lmBujfkD4+y0sTZ5qZlfvtqqQrGKZD\nbkT8dSE1MrPSKHWSA84atVpsxLr1+c/L2VfQX8iaAuoK8NzqVcXEfe7J3GNuvfWOuccEWLZ8yJXm\n2tJf0IPwgXXrco85cdy43GPmpdRJLiJ+MZoVMbPy6avCpJlmZkMpdUvOzKyREuS45pOcpHER8UKR\nlTGzkilBlmtmZuBZkm4H/pC+7ybp3wqvmZl1vaoM6/oGcDDwOEBE3Eq22LSZ9Tj1qemtU5q5Xe2L\niAfqMnH+78nNrHQ6OSa1Wc0kuWWSZgEhqR84GSim85KZlUpV3q5+gOyWdTvgMeDnaV/TJL0JmAXc\nERHXjLSSZtadKpHkImIFcPhIgkq6MSJmpc8nAB8ErgA+IWmPiPhCK5U1s+6i7r9bbWpm4PPYyBjW\niBhuKa1Naj7PBQ6MiJWSvky2RsRGk1ztal3nnHMOJ3i1LrPuVoWWHNnt6aBNgf8JLGtwTZ+kl5O9\nve2PiJUAEbFK0sBQF9Wu1rVu/Xqv1mXW5Srx4iEiNpjqXNJ3gWsbXDYFuIlsyvSQtHVEPCJpUtpn\nZhVQiWdyGzED2Gq4EyJi+yEOrSdrCZpZBZRhIZtmnsk9yUvP5PrIFps+rZXCImI1cH8r15pZ9yl9\nS07ZT7Ab2boOAOsjws/KzAwoR5Ib9qlhSmgLImJd2pzgzOxFUvNbc/E0W9I9kpZIGvKOUdLekgYk\nvbtRzGZejdwi6fXNVdHMeon6+5reGsbKRlSdTbbk6UzgCEkzhzjvi0BTAwuGW+NhTEQMAK8HFkq6\nD1hFemMaEXs0U4CZVVfOt6uzgCURsTTFvhQ4FLiz7ryTgcuAvZsJOtwzuRuBPYBDRlxVM+sJI0ly\ntZ39k/mpb+ygbdmwD+5y4A11MbYl66HxVnJIcgKIiPuaCZS3ohYaKcLY/v5C4o6bMLGQuEUsEvTg\n8ntyjwkwftMJhcRdu7aY+V/HFPS70K1GkuRqO/u34WvARyNifbNlD5fkpko6daiDEfGvI6ycmVVM\nzv3kHgKm13yfxks9OwbtBVyaEtwWwDskDUTEfwwVdLgk1w94hIKZDSnnZ3ILgZ0kzSBLbocDR9ae\nEBEzasr+FnDVcAkOhk9yj0TEp1qurplVXl+OLbmIGJB0EnA1WSPrwohYLOnEdHxeK3EbPpMzMxtS\nzp2BI2IBsKBu30aTW0Qc20zM4ZLcAU3XzMx6UqnHrkbEE6NZETMrnzIM6/Li0mbWMic5M6u0Skya\naWY2lDKs8VBIFSW9QdLL0ufxks6U9GNJX5Q0pYgyzWz0SWp665Si8vCFwOr0+etk06F/Me27qKAy\nzWy05T3XUgGKSnJ9aQYTgL0i4sMRcW1EnAnsMNRFkuZKWiRp0fz57Q5xM7OilaElV9QzuTskvT8i\nLgJulbRXRCyStDOwdqiL6gbweoJOsy7Xy29Xjwe+LukM4E/AdZKWkU2jcnxBZZrZKOtrYjLMTisk\nyUXE08Cx6eXDjFTO8oh4rIjyzKwzerklB0BEPAPcWmQZZtY5Jchx7idnZm0oQZZzkjOzlpV6gL6Z\nWSMe1mVmldbzLx7MrNqc5NoQkX9f4IH163KPCaCCJlEuauWnIv5sN+kv5lepqFW1pk6d3vikFqxY\n8WDuMZ9+fnXjk1q0WZsrwvmZnJlVWgkack5yZtaGEmQ5Jzkza5nfrppZpfmZnJlVmt+umlmlOcmZ\nWaU5yZlZpZUgxznJmVnrVIJJM4taretDkorpUm5mXaMMazwUlYY/Ddwg6TeS/l7S1GYu8kI2ZuVS\nhiRX1O3qUmBP4O3Ae4EzJd0EXAJcHhHPbuyi2oVsoogBlmaWq74SPJQrqiUXEbE+Iq6JiOOAbYBv\nArPJEqCZVUAvt+Q2+IkiYi1wJXClpAkFlWlmo6y/h0c8vHeoAxFR3LwxZjaqippmLE9FLUl4bxFx\nzay7lOGZnPvJmVnLyjDioft78plZ18r7xYOk2ZLukbRE0mkbOX6UpNsk3S7pd5J2axTTLTkza1me\nt6uS+oGzgQOB5cBCSVdGxJ01p90PvCUinpQ0h6zL2RuGi+skZ2Yt68930sxZwJKIWAog6VLgUODF\nJBcRv6s5/3pgWqOgvl01s5ZJI9leGtGUtrl14bYFltV8X572DeU44L8a1bGnWnIvrB0oJG5RD1/X\nFTToY9yY/P/aVzzzTO4xAaZOnlxI3JUrlzU+qQW77LJ37jFvveN3jU/qkJF0Iakd0dR2udJbyZLc\nmxqd21NJzszylXMXkoeA2ok9pqV9G5D0OuB8YE5EPN4oqG9XzaxlOb9dXQjsJGmGpLHA4WQjpWrL\n2w64HDi62f64bsmZWcvyfFQTEQOSTgKuBvqBCyNisaQT0/F5wL8AmwPfTGUPRMRew8V1kjOzluX8\ndpWIWAAsqNs3r+bz8cDxI4npJGdmLSvDiAcnOTNrWQkmIXGSM7PW9ewsJGbWG3p2FpKa178PR8TP\nJR0J7AvcBcxPk2iaWcn15fzioQhFteQuSrEnSHofMImsb8sBZOPT3ldQuWY2inq2JQf8ZUS8TtIY\nsh7L20TEOknfA24d6qI0lm0uwLx585g7t35om5l1k15+u9qXblknAhOAKcATwDhgk6Eu8mpdZuXS\ny0nuAuBusl7LpwM/lLQU2Ae4tKAyzWyU9WwXkoj4qqTvp88PS/oO2Rqs50XEjUWUaWajr6e7kETE\nwzWfnwJ+VFRZZtYZeQ/rKoL7yZlZy3r5mZyZ9YBe7kJiZj3ALTkzqzQnOTOrtJ7tQmJmvaFP3f92\nVV08sCD3iq1bvz7vkEBxD1/XF/R38/yaNbnH3HSTIQeytGVMf38hcVe98EIhcYv4c9hy6vTGJ7Xo\n8ccfauuXd+mKFU3/ku6w5ZYdafe5JWdmLfMzOTOrNHchMbNKc0vOzCqtvwSvV53kzKxlPT1A38yq\nz7erZlZpfvFgZpXW0y05STsAfw1MB9YB9wIXR8QzRZVpZqOrDEmukDEZkj4EzAM2BfYmW9thOnC9\npP2LKNPMRl9/X1/TW6cUVfIJwJyI+AzZtOevjYjTgdnAV4e6SNJcSYskLZo/f35BVTOzvPSp+a1T\ninwmN4bsNnUc2bqrRMSDkpparYsCxq6aWb56uQvJ+cBCSTcAbwa+CCBpKtnShGZWAWV4JlfUal1f\nl/Rz4DXAVyLi7rR/JbBfEWWa2ejr6S4kEbEYWFxUfDPrvJ5tyZlZb/CShGZWaW7JmVmllWASksL6\nyZlZD9AI/tdUPGm2pHskLZF02kaOS9I30vHbJO3RKKaTnJm1TFLTWxOx+oGzgTnATOAISTPrTpsD\n7JS2ucA5jeL6dtXMWpbzi4dZwJKIWAog6VLgUODOmnMOBb4T2Qpc10vaTNLWEfHIkFEjovQbMLfX\n45aprmWLW6a6Fhk3j3oBi2q2uXXH3w2cX/P9aOCsunOuAt5U8/0XwF7DlVuV29W5jluqupYtbpnq\nWmTctkTE/IjYq2YblQHqVUlyZlZ+D5HNVjRoWto30nM24CRnZt1iIbCTpBmSxgKHA1fWnXMlcEx6\ny7oP8HQM9zyO6rx4KKrZW6a4Zapr2eKWqa5Fxi1URAxIOgm4GugHLoyIxZJOTMfnAQuAdwBLgNXA\n+xvFVXp4Z2ZWSb5dNbNKc5Izs0orfZJrNAykxZgXSloh6Y484qWY0yX9t6Q7JS2WdEpOcTeVdKOk\nW1PcM/OIm2L3S/q9pKtyjPlHSbdLukXSohzjbibpR5LulnSXpP+RQ8xdUj0Ht2ckfTiHuP+Q/q7u\nkHSJpE3bjZninpJiLs6jnpXR6Q6CbXYu7AfuA3YAxgK3AjNziLsfsAdwR4513RrYI32eTLZ6WR51\nFTApfd4EuAHYJ6c6nwpcDFyV45/DH4EtCvhd+DZwfPo8FtisgN+1R4FXthlnW+B+YHz6/gPg2Bzq\ntytwBzCB7IXiz4FX5f3nXMat7C25F4eBRMQaYHAYSFsi4tfkPE17RDwSETenz88Cd5H9wrcbNyLi\nufR1k7S1/TZJ0jTgnWRT2Xc1SVPI/mG6ACAi1kTEUzkXcwBwX0Q8kEOsMcB4SWPIktLDOcR8DXBD\nRKyOiAHgV2RLgva8sie5bYFlNd+Xk0PiKJqk7YHXk7W68ojXL+kWYAXws4jII+7XgH8C1ucQq1YA\nP5d0k6S8eubPAFYCF6Xb6/MlTcwp9qDDgUvaDRIRDwFfBh4EHiHr53VNu3HJWnFvlrS5pAlk3Sym\nN7imJ5Q9yZWOpEnAZcCHI6eFtiNiXUTsTtb7e5akXdus48HAioi4KY/61XlTqusc4IOS8ljzYwzZ\n44VzIuL1wCogl+ezAKlj6iHAD3OI9XKyu40ZwDbAREl/227ciLiLbMGoa4CfAreQrZbX88qe5EY8\nxKOT0nKMlwH/HhGX5x0/3aL9N9n6tu14I3CIpD+SPQJ4m6TvtRkTeLElQ0SsAK4ge+TQruXA8poW\n7I/Ikl5e5gA3R8RjOcR6O3B/RKyMiLXA5cC+OcQlIi6IiD0jYj/gSbLnvj2v7EmumWEgXUHZhFoX\nAHdFxL/mGHeqpM3S5/HAgcDd7cSMiI9FxLSI2J7sz/T/RkTbrQ1JEyVNHvwMHER2m9WWiHgUWCZp\nl7TrADacnqddR5DDrWryILCPpAnpd+IAsuezbZO0ZfrvdmTP4y7OI27ZlXpYVwwxDKTduJIuAfYH\ntpC0HPhERFzQZtg3kk0dc3t6fgbw8YhY0GbcrYFvpwkH+4AfRERuXT5ythVwRZpAcQxwcUT8NKfY\nJwP/nv6xW0oTw32akZLxgcD/yiNeRNwg6UfAzcAA8HvyG4Z1maTNgbXABwt4+VJKHtZlZpVW9ttV\nM7NhOcmZWaU5yZlZpTnJmVmlOcmZWaU5yZWYpHVpdow7JP0wDedpNdb+g7ONSDpkuBld0owff99C\nGZ+U9I/N7q8751uS3j2CsrbPcxYZKy8nuXJ7PiJ2j4hdgTXAibUH0zz4I/47jogrI+ILw5yyGTDi\nJGfWCU5y1fEb4FWpBXOPpO+QjSaYLukgSddJujm1+CbBi3Px3S3pZmpmrJB0rKSz0uetJF2R5qu7\nVdK+wBeAHVMr8kvpvI9IWijptto57SSdLuleSdcCu9CApBNSnFslXVbXOn27pEUp3sHp/H5JX6op\nO5dOu1YdTnIVkKbsmQPcnnbtBHwzIl5LNlj9DODtEbEH2aK+p6aJGs8D3gXsCfzFEOG/AfwqInYj\nGw+6mGzw+32pFfkRSQelMmcBuwN7StpP0p5kw8J2J5sVY+8mfpzLI2LvVN5dwHE1x7ZPZbwTmJd+\nhuPIZvLYO8U/QdKMJsqxHlHqYV3G+JohYr8hGxu7DfBARFyf9u8DzAR+m4ZTjQWuA15NNlD8DwBp\nAP7Gpj56G3AMZLOdAE+nmTRqHZS236fvk8iS3mTgiohYncpoZlzxrpI+Q3ZLPIlsyN6gH0TEeuAP\nkpamn+Eg4HU1z+umpLI9ON0AJ7myez5NW/SilMhW1e4im2PuiLrzNriuTQI+HxHn1pXRyhTc3wIO\ni4hbJR1LNoZ4UP0YxEhlnxwRtclwcM4+M9+u9oDrgTdKehW8OBPIzmQzlWwvacd03hFDXP8L4APp\n2n5ls/A+S9ZKG3Q18Hc1z/q2TTNi/Bo4TNL4NPvIu5qo72TgkTQt1VF1x94jqS/VeQfgnlT2B9L5\nSNpZ+U+YaSXmllzFRcTK1CK6RNK4tPuMiLhX2cy8P5G0mux2d/JGQpwCzJd0HNkkjB+IiOsk/TZ1\n0fiv9FzuNcB1qSX5HPC3EXGzpO+Trb2xgmxqrEb+mWzG5JXpv7V1ehC4EXgZcGJE/FnS+WTP6m5O\nUxetBA5r7k/HeoFnITGzSvPtqplVmpOcmVWak5yZVZqTnJlVmpOcmVWak5yZVZqTnJlV2v8Dijju\nsI5zSX4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131105e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = train_linear_classification_model(\n",
    "    learning_rate=0.03,\n",
    "    steps=1000,\n",
    "    batch_size=30,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Replace the Linear Classifier with a Neural Network\n",
    "\n",
    "**Replace the LinearClassifier above with a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and find a parameter combination that gives 0.95 or better accuracy.**\n",
    "\n",
    "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1       2       3       4       5       6       7       8       9    \\\n",
       "count 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          10    ...       775     776     777     778     779     780     781  \\\n",
       "count 10000.0   ...   10000.0 10000.0 10000.0 10000.0 10000.0 10000.0 10000.0   \n",
       "mean      0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0   ...       0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0   ...       1.0     1.0     0.6     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          782     783     784  \n",
       "count 10000.0 10000.0 10000.0  \n",
       "mean      0.0     0.0     0.0  \n",
       "std       0.0     0.0     0.0  \n",
       "min       0.0     0.0     0.0  \n",
       "25%       0.0     0.0     0.0  \n",
       "50%       0.0     0.0     0.0  \n",
       "75%       0.0     0.0     0.0  \n",
       "max       0.0     0.0     0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test_dataframe = pd.read_csv(\n",
    "  \"https://storage.googleapis.com/mledu-datasets/mnist_test.csv\",\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
    "test_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
    "\n",
    "    In addition to training, this function also prints training progress information,\n",
    "    a plot of the training and validation loss over time, as well as a confusion\n",
    "    matrix.\n",
    "\n",
    "    Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "\n",
    "    Returns:\n",
    "    The trained `DNNClassifier` object.\n",
    "    \"\"\"\n",
    "\n",
    "    periods = 10\n",
    "    # Caution: input pipelines are reset with each call to train. \n",
    "    # If the number of steps is small, your model may never see most of the data.  \n",
    "    # So with multiple `.train` calls like this you may want to control the length \n",
    "    # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
    "    # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
    "    steps_per_period = steps / periods  \n",
    "    # Create the input functions.\n",
    "    predict_training_input_fn = create_predict_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "    predict_validation_input_fn = create_predict_input_fn(\n",
    "    validation_examples, validation_targets, batch_size)\n",
    "    training_input_fn = create_training_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "\n",
    "    # Create the input functions.\n",
    "    predict_training_input_fn = create_predict_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "    predict_validation_input_fn = create_predict_input_fn(\n",
    "    validation_examples, validation_targets, batch_size)\n",
    "    training_input_fn = create_training_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "\n",
    "    # Create feature columns.\n",
    "    feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
    "\n",
    "    # Create a DNNClassifier object.\n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      n_classes=10,\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer,\n",
    "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
    "    )\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss error (on validation data):\")\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "\n",
    "        # Take a break and compute probabilities.\n",
    "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
    "\n",
    "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
    "\n",
    "        # Compute training and validation errors.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "    print(\"Model training finished.\")\n",
    "    # Remove event files to save disk space.\n",
    "    _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "\n",
    "    # Calculate final predictions (not probabilities, as above).\n",
    "    final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Output a plot of the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class).\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "    ax.set_aspect(1)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss error (on validation data):\n",
      "  period 00 : 4.77\n"
     ]
    }
   ],
   "source": [
    "classifier = train_nn_classification_model(\n",
    "    learning_rate=0.05,\n",
    "    steps=1000,\n",
    "    batch_size=30,\n",
    "    hidden_units=[100, 100],\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_dataframe = pd.read_csv(\n",
    "  \"https://storage.googleapis.com/mledu-datasets/mnist_test.csv\",\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
    "test_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_input_fn = create_predict_input_fn(\n",
    "    test_examples, test_targets, batch_size=100)\n",
    "\n",
    "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
    "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
    "  \n",
    "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
    "print(\"Accuracy on test data: %0.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualize the weights of the first hidden layer.\n",
    "\n",
    "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
    "\n",
    "The input layer of our model has `784` weights corresponding to the `28×28` pixel input images. The first hidden layer will have `784×N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28×28` images by *reshaping* each of the `N` `1×784` arrays of weights into `N` arrays of size `28×28`.\n",
    "\n",
    "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.get_variable_names())\n",
    "\n",
    "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
    "\n",
    "print(\"weights0 shape:\", weights0.shape)\n",
    "\n",
    "num_nodes = weights0.shape[1]\n",
    "num_rows = int(math.ceil(num_nodes / 10.0))\n",
    "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
    "for coef, ax in zip(weights0.T, axes.ravel()):\n",
    "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
    "\n",
    "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
    "\n",
    "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
    "\n",
    "What differences do you see visually for the different levels of convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
