{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings: Motivation From Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborative filtering** is the task of making predictions about the interests of a user based on interests of many other users. As an example, let's look at the task of movie recommendation. Suppose we have 1,000,000 users, and a list of the movies each user has watched (from a catalog of 500,000 movies). Our goal is to recommend movies to users.\n",
    "\n",
    "To solve this problem some method is needed to determine which movies are similar to each other. We can achieve this goal by embedding the movies into a low-dimensional space created such that similar movies are nearby.\n",
    "\n",
    "Before describing how we can learn the embedding, we first explore the type of qualities we want the embedding to have, and how we will represent the training data for learning the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange Movies on a One-Dimensional Number Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help develop intuition about embeddings, on a piece of paper, try to arrange the following movies on a one-dimensional number line so that the movies nearest each other are the most closely related:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/one-di.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange Movies in a Two-Dimensional Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the same exercise as before, but this time arrange the same movies in a two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/two-di.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Categorical Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data** refers to input features that represent one or more discrete items from a finite set of choices. For example, it can be the set of movies a user has watched, the set of words in a document, or the occupation of a person.\n",
    "\n",
    "Categorical data is most efficiently represented via **sparse tensors**, which are tensors with **very few non-zero elements**. For example, if we're building a movie recommendation model, we can assign a unique ID to each possible movie, and then represent each user by a sparse tensor of the movies they have watched, as shown in Figure 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/data_movie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the matrix in Figure 3 is an example capturing a user's movie-viewing history, and is represented as a sparse tensor because each user only watches a small fraction of all possible movies. The last row corresponds to the sparse tensor [1, 3, 999999], using the vocabulary indices shown above the movie icons.\n",
    "\n",
    "Likewise one can represent words, sentences, and documents as sparse vectors where each word in the vocabulary plays a role similar to the movies in our recommendation example.\n",
    "\n",
    "In order to use such representations within a machine learning system, we need a way to represent each sparse vector as a vector of numbers so that semantically similar items (movies or words) have similar distances in the vector space. But how do you represent a word as a vector of numbers?\n",
    "\n",
    "The simplest way is to define a giant input layer with a node for every word in your vocabulary, or at least a node for every word that appears in your data. If 500,000 unique words appear in your data, you could represent a word with a length 500,000 vector and assign each word to a slot in the vector.\n",
    "\n",
    "If you assign \"horse\" to index 1247, then to feed \"horse\" into your network you might copy a 1 into the 1247th input node and 0s into all the rest. This sort of representation is called a **one-hot encoding**, because only one index has a non-zero value.\n",
    "\n",
    "More typically your vector might contain counts of the words in a larger chunk of text. This is known as a \"bag of words\" representation. In a bag-of-words vector, several of the 500,000 nodes would have non-zero value.\n",
    "\n",
    "But however you determine the non-zero values, one-node-per-word gives you very sparse input vectorsâ€”very large vectors with relatively few non-zero values. Sparse representations have a couple of problems that can make it hard for a model to learn effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge input vectors mean a super-huge number of weights for a neural network. If there are M words in your vocabulary and N nodes in the first layer of the network above the input, you have MxN weights to train for that layer. A large number of weights causes further problems:\n",
    "\n",
    "- **Amount of data**. The more weights in your model, the more data you need to train effectively.\n",
    "- **Amount of computation**. The more weights, the more computation required to train and use the model. It's easy to exceed the capabilities of your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of Meaningful Relations Between Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feed the pixel values of RGB channels into an image classifier, it makes sense to talk about \"close\" values. Reddish blue is close to pure blue, both semantically and in terms of the geometric distance between vectors. But a vector with a 1 at index 1247 for \"horse\" is not any closer to a vector with a 1 at index 50,430 for \"antelope\" than it is to a vector with a 1 at index 238 for \"television\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to these problems is to use **embeddings**, which translate large sparse vectors into a lower-dimensional space that preserves semantic relationships. We'll explore embeddings intuitively, conceptually, and programmatically in the following sections of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating to a Lower-Dimensional Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can solve the core problems of sparse input data by mapping your high-dimensional data into a lower-dimensional space.\n",
    "\n",
    "As you can see from the paper exercises, even a small multi-dimensional space provides the freedom to group semantically similar items together and keep dissimilar items far apart. Position (distance and direction) in the vector space can encode semantics in a good embedding. For example, the following visualizations of real embeddings show geometrical relationships that capture semantic relations like the relation between a country and its capital:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/dim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of meaningful space gives your machine learning system opportunities to detect patterns that may help with the learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinking the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we want enough dimensions to encode rich semantic relations, we also want an embedding space that is small enough to allow us to train our system more quickly. A useful embedding may be on the order of hundreds of dimensions. This is likely several orders of magnitude smaller than the size of your vocabulary for a natural language task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings as lookup tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding is a matrix in which each column is the vector that corresponds to an item in your vocabulary. To get the dense vector for a single vocabulary item, you retrieve the column corresponding to that item.\n",
    "\n",
    "But how would you translate a sparse bag of words vector? To get the dense vector for a sparse vector representing multiple vocabulary items (all the words in a sentence or paragraph, for example), you could retrieve the embedding for each individual item and then add them together.\n",
    "\n",
    "If the sparse vector contains counts of the vocabulary items, you could multiply each embedding by the count of its corresponding item before adding it to the sum.\n",
    "\n",
    "These operations may look familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding lookup as matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lookup, multiplication, and addition procedure we've just described is equivalent to matrix multiplication. Given a 1 X N sparse representation S and an N X M embedding table E, the matrix multiplication S X E gives you the 1 X M dense vector.\n",
    "\n",
    "But how do you get E in the first place? We'll take a look at how to obtain embeddings in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many existing mathematical techniques for capturing the important structure of a high-dimensional space in a low dimensional space. In theory, any of these techniques could be used to create an embedding for a machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, [principal component analysis](https://wikipedia.org/wiki/Principal_component_analysis) (PCA) has been used to create word embeddings. Given a set of instances like bag of words vectors, PCA tries to find highly correlated dimensions that can be collapsed into a single dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is an algorithm invented at Google for training word embeddings. Word2vec relies on the **distributional hypothesis** to map semantically similar words to geometrically close embedding vectors.\n",
    "\n",
    "The distributional hypothesis states that words which often have the same neighboring words tend to be semantically similar. Both \"dog\" and \"cat\" frequently appear close to the word \"vet\", and this fact reflects their semantic similarity. As the linguist John Firth put it in 1957, \"You shall know a word by the company it keeps\".\n",
    "\n",
    "Word2Vec exploits contextual information like this by training a neural net to distinguish actually co-occurring groups of words from randomly grouped words. The input layer takes a sparse representation of a target word together with one or more context words. This input connects to a single, smaller hidden layer.\n",
    "\n",
    "In one version of the algorithm, the system makes a negative example by substituting a random noise word for the target word. Given the positive example \"the plane flies\", the system might swap in \"jogging\" to create the contrasting negative example \"the jogging flies\".\n",
    "\n",
    "The other version of the algorithm creates negative examples by pairing the true target word with randomly chosen context words. So it might take the positive examples (the, plane), (flies, plane) and the negative examples (compiled, plane), (who, plane) and learn to identify which pairs actually appeared together in text.\n",
    "\n",
    "The classifier is not the real goal for either version of the system, however. After the model has been trained, you have an embedding. You can use the weights connecting the input layer with the hidden layer to map sparse representations of words to smaller vectors. This embedding can be reused in other classifiers.\n",
    "\n",
    "For more information about word2vec, see the [tutorial on tensorflow.org](https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Embedding as Part of a Larger Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also learn an **embedding as part of the neural network** for your target task. This approach gets you an embedding well customized for your particular system, but may take longer than training the embedding separately.\n",
    "\n",
    "In general, when you have sparse data (or dense data that you'd like to embed), you can create an embedding unit that is just a special type of hidden unit of size $d$. This embedding layer can be combined with any other features and hidden layers. As in any DNN, the final layer will be the loss that is being optimized. \n",
    "\n",
    "For example, let's say we're performing collaborative filtering, where the goal is to predict a user's interests from the interests of other users. We can model this as a supervised learning problem by randomly setting aside (or holding out) a small number of the movies that the user has watched as the positive labels, and then optimize a softmax loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/embed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example if you want to create an embedding layer for the words in a real-estate ad as part of a DNN to predict housing prices then you'd optimize an L2 Loss using the known sale price of homes in your training data as the label.\n",
    "\n",
    "When learning a d-dimensional embedding each item is mapped to a point in a d-dimensional space so that the similar items are nearby in this space. Figure 6 helps to illustrate the relationship between the weights learned in the embedding layer and the geometric view. The edge weights between an input node and the nodes in the d-dimensional embedding layer correspond to the coordinate values for each of the d axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/embed2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Convert movie-review string data to a sparse feature vector\n",
    "* Implement a sentiment-analysis linear model using a sparse feature vector\n",
    "* Implement a sentiment-analysis DNN model using an embedding that projects data into two dimensions\n",
    "* Visualize the embedding to see what the model has learned about the relationships between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import our dependencies and download the training and test data. [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) includes a file download and caching tool that we can use to retrieve the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/train.tfrecord\n",
      "41631744/41625533 [==============================] - 31s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/test.tfrecord\n",
      "40689664/40688441 [==============================] - 38s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import io\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "train_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/train.tfrecord'\n",
    "train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)\n",
    "test_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/test.tfrecord'\n",
    "test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sentiment Analysis Model\n",
    "Let's train a sentiment-analysis model on this data that predicts if a review is generally *favorable* (label of 1) or *unfavorable* (label of 0).\n",
    "\n",
    "To do so, we'll turn our string-value `terms` into feature vectors by using a *vocabulary*, a list of each term we expect to see in our data. For the purposes of this exercise, we've created a small vocabulary that focuses on a limited set of terms. Most of these terms were found to be strongly indicative of *favorable* or *unfavorable*, but some were just added because they're interesting.\n",
    "\n",
    "Each term in the vocabulary is mapped to a coordinate in our feature vector. To convert the string-value `terms` for an example into this vector format, we encode such that each coordinate gets a value of 0 if the vocabulary term does not appear in the example string, and a value of 1 if it does. Terms in an example that don't appear in the vocabulary are thrown away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *We could of course use a larger vocabulary, and there are special tools for creating these. In addition, instead of just dropping terms that are not in the vocabulary, we can introduce a small number of OOV (out-of-vocabulary) buckets to which you can hash the terms not in the vocabulary. We can also use a __feature hashing__ approach that hashes each term, instead of creating an explicit vocabulary. This works well in practice, but loses interpretability, which is useful for this exercise. See see the tf.feature_column module for tools handling this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's configure the input pipeline to import our data into a TensorFlow model. We can use the following function to parse the training and test data (which is in [TFRecord](https://www.tensorflow.org/programmers_guide/datasets) format) and return a dict of the features and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(record):\n",
    "    \"\"\"Extracts features and labels.\n",
    "\n",
    "    Args:\n",
    "    record: File path to a TFRecord file    \n",
    "    Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      features: A dict of tensors representing the features\n",
    "      labels: A tensor with the corresponding labels.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "    \"terms\": tf.VarLenFeature(dtype=tf.string), # terms are strings of varying lengths\n",
    "    \"labels\": tf.FixedLenFeature(shape=[1], dtype=tf.float32) # labels are 0 or 1\n",
    "    }\n",
    "\n",
    "    parsed_features = tf.parse_single_example(record, features)\n",
    "\n",
    "    terms = parsed_features['terms'].values\n",
    "    labels = parsed_features['labels']\n",
    "\n",
    "    return  {'terms':terms}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm our function is working as expected, let's construct a `TFRecordDataset` for the training data, and map the data to features and labels using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ({terms: (?,)}, (1,)), types: ({terms: tf.string}, tf.float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Dataset object.\n",
    "ds = tf.data.TFRecordDataset(train_path)\n",
    "# Map features and labels with the parse function.\n",
    "ds = ds.map(_parse_function)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to retrieve the first example from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'terms': array([b'but', b'it', b'does', b'have', b'some', b'good', b'action',\n",
       "         b'and', b'a', b'plot', b'that', b'is', b'somewhat', b'interesting',\n",
       "         b'.', b'nevsky', b'acts', b'like', b'a', b'body', b'builder',\n",
       "         b'and', b'he', b'isn', b\"'\", b't', b'all', b'that', b'attractive',\n",
       "         b',', b'in', b'fact', b',', b'imo', b',', b'he', b'is', b'ugly',\n",
       "         b'.', b'(', b'his', b'acting', b'skills', b'lack', b'everything',\n",
       "         b'!', b')', b'sascha', b'is', b'played', b'very', b'well', b'by',\n",
       "         b'joanna', b'pacula', b',', b'but', b'she', b'needed', b'more',\n",
       "         b'lines', b'than', b'she', b'was', b'given', b',', b'her',\n",
       "         b'character', b'needed', b'to', b'be', b'developed', b'.',\n",
       "         b'there', b'are', b'way', b'too', b'many', b'men', b'in', b'this',\n",
       "         b'story', b',', b'there', b'is', b'zero', b'romance', b',', b'too',\n",
       "         b'much', b'action', b',', b'and', b'way', b'too', b'dumb', b'of',\n",
       "         b'an', b'ending', b'.', b'it', b'is', b'very', b'violent', b'.',\n",
       "         b'i', b'did', b'however', b'love', b'the', b'scenery', b',',\n",
       "         b'this', b'movie', b'takes', b'you', b'all', b'over', b'the',\n",
       "         b'world', b',', b'and', b'that', b'is', b'a', b'bonus', b'.', b'i',\n",
       "         b'also', b'liked', b'how', b'it', b'had', b'some', b'stuff',\n",
       "         b'about', b'the', b'mafia', b'in', b'it', b',', b'not', b'too',\n",
       "         b'much', b'or', b'too', b'little', b',', b'but', b'enough',\n",
       "         b'that', b'it', b'got', b'my', b'attention', b'.', b'the',\n",
       "         b'actors', b'needed', b'to', b'be', b'more', b'handsome', b'.',\n",
       "         b'.', b'.', b'the', b'biggest', b'problem', b'i', b'had', b'was',\n",
       "         b'that', b'nevsky', b'was', b'just', b'too', b'normal', b',',\n",
       "         b'not', b'sexy', b'enough', b'.', b'i', b'think', b'for', b'most',\n",
       "         b'guys', b',', b'sascha', b'will', b'be', b'hot', b'enough', b',',\n",
       "         b'but', b'for', b'us', b'ladies', b'that', b'are', b'fans', b'of',\n",
       "         b'action', b',', b'nevsky', b'just', b'doesn', b\"'\", b't', b'cut',\n",
       "         b'it', b'.', b'overall', b',', b'this', b'movie', b'was', b'fine',\n",
       "         b',', b'i', b'didn', b\"'\", b't', b'love', b'it', b'nor', b'did',\n",
       "         b'i', b'hate', b'it', b',', b'just', b'found', b'it', b'to', b'be',\n",
       "         b'another', b'normal', b'action', b'flick', b'.'], dtype=object)},\n",
       " array([0.], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = ds.make_one_shot_iterator().get_next()\n",
    "sess = tf.Session()\n",
    "sess.run(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a formal input function that we can pass to the `train()` method of a TensorFlow Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input_fn that parses the tf.Examples from the given files,\n",
    "# and split them into features and targets.\n",
    "def _input_fn(input_filenames, num_epochs=None, shuffle=True):\n",
    "    # Same code as above; create a dataset and map features and labels.\n",
    "    ds = tf.data.TFRecordDataset(input_filenames)\n",
    "    ds = ds.map(_parse_function)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    # Our feature data is variable-length, so we pad and batch\n",
    "    # each field of the dataset structure to whatever size is necessary.\n",
    "    ds = ds.padded_batch(25, ds.output_shapes)\n",
    "    ds = ds.repeat(num_epochs)\n",
    "\n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Use a Linear Model with Sparse Inputs and an Explicit Vocabulary\n",
    "\n",
    "For our first model, we'll build a [`LinearClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier) model using 50 informative terms; always start simple!\n",
    "\n",
    "The following code constructs the feature column for our terms. The [`categorical_column_with_vocabulary_list`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list) function creates a feature column with the string-to-feature-vector mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 informative terms that compose our model vocabulary \n",
    "informative_terms = (\"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                     \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                     \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                     \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                     \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                     \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                     \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                     \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                     \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                     \"drama\", \"family\")\n",
    "\n",
    "terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=\"terms\", vocabulary_list=informative_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll construct the `LinearClassifier`, train it on the training set, and evaluate it on the evaluation set. After you read through the code, run it and see how you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set metrics:\n",
      "accuracy 0.78792\n",
      "accuracy_baseline 0.5\n",
      "auc 0.8718428\n",
      "auc_precision_recall 0.8626741\n",
      "average_loss 0.45132965\n",
      "label/mean 0.5\n",
      "loss 11.283241\n",
      "precision 0.75438225\n",
      "prediction/mean 0.51575255\n",
      "recall 0.85384\n",
      "global_step 1000\n",
      "---\n",
      "Test set metrics:\n",
      "accuracy 0.78524\n",
      "accuracy_baseline 0.5\n",
      "auc 0.8704506\n",
      "auc_precision_recall 0.86055905\n",
      "average_loss 0.45150518\n",
      "label/mean 0.5\n",
      "loss 11.28763\n",
      "precision 0.7522105\n",
      "prediction/mean 0.51452655\n",
      "recall 0.85072\n",
      "global_step 1000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "feature_columns = [ terms_feature_column ]\n",
    "\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  optimizer=my_optimizer,\n",
    ")\n",
    "\n",
    "classifier.train(\n",
    "  input_fn=lambda: _input_fn([train_path]),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn([train_path]),\n",
    "  steps=1000)\n",
    "print(\"Training set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "    print(m, evaluation_metrics[m])\n",
    "print(\"---\")\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn([test_path]),\n",
    "  steps=1000)\n",
    "\n",
    "print(\"Test set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "    print(m, evaluation_metrics[m])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use a Deep Neural Network (DNN) Model\n",
    "\n",
    "The above model is a linear model.  It works quite well.  But can we do better with a DNN model?\n",
    "\n",
    "Let's swap in a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) for the `LinearClassifier`. Run the following cell, and see how you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set metrics:\n",
      "accuracy 0.84\n",
      "accuracy_baseline 0.6\n",
      "auc 0.8633332\n",
      "auc_precision_recall 0.8949144\n",
      "average_loss 0.41597623\n",
      "label/mean 0.6\n",
      "loss 10.3994055\n",
      "precision 0.8235294\n",
      "prediction/mean 0.6132659\n",
      "recall 0.93333334\n",
      "global_step 1000\n",
      "---\n",
      "Test set metrics:\n",
      "accuracy 0.88\n",
      "accuracy_baseline 0.64\n",
      "auc 0.986111\n",
      "auc_precision_recall 0.97515416\n",
      "average_loss 0.2922978\n",
      "label/mean 0.36\n",
      "loss 7.307445\n",
      "precision 0.875\n",
      "prediction/mean 0.43517616\n",
      "recall 0.7777778\n",
      "global_step 1000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "##################### Here's what we changed ##################################\n",
    "classifier = tf.estimator.DNNClassifier(                                      #\n",
    "  feature_columns=[tf.feature_column.indicator_column(terms_feature_column)], #\n",
    "  hidden_units=[20,20],                                                       #\n",
    "  optimizer=my_optimizer,                                                     #\n",
    ")                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "try:\n",
    "    classifier.train(\n",
    "        input_fn=lambda: _input_fn([train_path]),\n",
    "        steps=1000)\n",
    "\n",
    "    evaluation_metrics = classifier.evaluate(\n",
    "        input_fn=lambda: _input_fn([train_path]),\n",
    "        steps=1)\n",
    "    print(\"Training set metrics:\")\n",
    "    for m in evaluation_metrics:\n",
    "        print(m, evaluation_metrics[m])\n",
    "    print(\"---\")\n",
    "\n",
    "    evaluation_metrics = classifier.evaluate(\n",
    "        input_fn=lambda: _input_fn([test_path]),\n",
    "        steps=1)\n",
    "\n",
    "    print(\"Test set metrics:\")\n",
    "    for m in evaluation_metrics:\n",
    "        print(m, evaluation_metrics[m])\n",
    "    print(\"---\")\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Use an Embedding with a DNN Model\n",
    "\n",
    "In this task, we'll implement our DNN model using an embedding column. An embedding column takes sparse data as input and returns a lower-dimensional dense vector as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, do the following:\n",
    "\n",
    "* Define the feature columns for the model using an `embedding_column` that projects the data into 2 dimensions (see the [TF docs](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) for more details on the function signature for `embedding_column`).\n",
    "* Define a `DNNClassifier` with the following specifications:\n",
    "  * Two hidden layers of 20 units each\n",
    "  * Adagrad optimization with a learning rate of 0.1\n",
    "  * A `gradient_clip_norm` of 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set metrics:\n",
      "accuracy 0.78388\n",
      "accuracy_baseline 0.5\n",
      "auc 0.86651176\n",
      "auc_precision_recall 0.8548876\n",
      "average_loss 0.45677468\n",
      "label/mean 0.5\n",
      "loss 11.419367\n",
      "precision 0.78272647\n",
      "prediction/mean 0.4886793\n",
      "recall 0.78592\n",
      "global_step 1000\n",
      "---\n",
      "Test set metrics:\n",
      "accuracy 0.78096\n",
      "accuracy_baseline 0.5\n",
      "auc 0.86584395\n",
      "auc_precision_recall 0.8525629\n",
      "average_loss 0.45761272\n",
      "label/mean 0.5\n",
      "loss 11.440318\n",
      "precision 0.7810499\n",
      "prediction/mean 0.4880887\n",
      "recall 0.7808\n",
      "global_step 1000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "########################## YOUR CODE HERE ######################################\n",
    "terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)\n",
    "feature_columns = [ terms_embedding_column ]\n",
    "\n",
    "my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=[20,20],\n",
    "  optimizer=my_optimizer\n",
    ")\n",
    "################################################################################\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: _input_fn([train_path]),\n",
    "    steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "    input_fn=lambda: _input_fn([train_path]),\n",
    "    steps=1000)\n",
    "print(\"Training set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "    print(m, evaluation_metrics[m])\n",
    "print(\"---\")\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "    input_fn=lambda: _input_fn([test_path]),\n",
    "    steps=1000)\n",
    "\n",
    "print(\"Test set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "    print(m, evaluation_metrics[m])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Convince yourself there's actually an embedding in there\n",
    "\n",
    "The above model used an `embedding_column`, and it seemed to work, but this doesn't tell us much about what's going on internally. How can we check that the model is actually using an embedding inside?\n",
    "\n",
    "To start, let's look at the tensors in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dnn/hiddenlayer_0/bias',\n",
       " 'dnn/hiddenlayer_0/bias/t_0/Adagrad',\n",
       " 'dnn/hiddenlayer_0/kernel',\n",
       " 'dnn/hiddenlayer_0/kernel/t_0/Adagrad',\n",
       " 'dnn/hiddenlayer_1/bias',\n",
       " 'dnn/hiddenlayer_1/bias/t_0/Adagrad',\n",
       " 'dnn/hiddenlayer_1/kernel',\n",
       " 'dnn/hiddenlayer_1/kernel/t_0/Adagrad',\n",
       " 'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights',\n",
       " 'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad',\n",
       " 'dnn/logits/bias',\n",
       " 'dnn/logits/bias/t_0/Adagrad',\n",
       " 'dnn/logits/kernel',\n",
       " 'dnn/logits/kernel/t_0/Adagrad',\n",
       " 'global_step']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_variable_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can see that there is an embedding layer in there: `'dnn/input_from_feature_columns/input_layer/terms_embedding/...'`. (What's interesting here, by the way, is that this layer is trainable along with the rest of the model just as any hidden layer is.)\n",
    "\n",
    "Is the embedding layer the correct shape? Run the following code to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** *Remember, in our case, the embedding is a matrix that allows us to project a 50-dimensional vector down to 2 dimensions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_variable_value('dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time manually checking the various layers and shapes to make sure everything is connected the way you would expect it would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Examine the Embedding\n",
    "\n",
    "Let's now take a look at the actual embedding space, and see where the terms end up in it. Do the following:\n",
    "1. Run the following code to see the embedding we trained in **Task 3**. Do things end up where you'd expect?\n",
    "\n",
    "2. Re-train the model by rerunning the code in **Task 3**, and then run the embedding visualization below again. What stays the same? What changes?\n",
    "\n",
    "3. Finally, re-train the model again using only 10 steps (which will yield a terrible model). Run the embedding visualization below again. What do you see now, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xtcj/f/+PHHuwPpIIeSU4R10umdQlTKOWMMsYYp5sxM\n2wzbh5y23zbGZ+xgTmVOazKMMVtkQg7FW0STiMhIq3TW4fX7o2/XR4SQU3vdb7dut97X+7pe1+t6\nc3u+r17X6/V8qoQQSJIkSdWH1vPugCRJklS1ZGCXJEmqZmRglyRJqmZkYJckSapmZGCXJEmqZmRg\nlyRJqmZkYJckSapmZGCXJEmqZmRglyRJqmZ0nsdJTUxMhIWFxfM4tSRJ0ksrJibmphDC9GH7PZfA\nbmFhQXR09PM4tSRJ0ktLpVJdqsx+cihGkiSpmpGBXZIkqZqRgV2SJKmakYFdkiSpmpGBXZIkqZqR\ngV2SJKmakYFdkiSpmpGBXZIkqZqRgV2SJKmakYFdkiSpmpGBXZIkqZqRgV2SJKmakYFdkiSpmpGB\nXZIkqZqRgV2SJKmakYFdkiSpmpGBXZIkqZqRgV2SJKmakYFdkiSpmpGBXZIkqZqRgV2SJKmakYFd\nkiSpmpGBXZIkqZqRgV2SJKmakYFdkiSpmpGBXZLukpSUhL29/WMfP2vWLMLDw6uwR5L0aHSedwck\nqTopLi5m7ty5z7sb0r+cvGOXpAoUFRUxdOhQbG1t8fX1JTc3lz179uDs7IyDgwMjR46koKAAAAsL\nC6ZNm0abNm3YtGkTAQEBhIWFKe8FBQXRpk0bHBwciI+PByA1NZXu3btjZ2fHqFGjaN68OTdv3nxu\n1ytVLzKwS1IF/vrrLyZMmMDZs2epXbs2ixYtIiAggNDQUE6dOkVRURHfffedsn/9+vU5fvw4fn5+\n97RlYmLC8ePHGT9+PAsXLgRgzpw5dOnShbi4OHx9fbl8+fIzuzap+pOBXZIqYG5ujru7OwDDhg1j\nz549tGjRAisrKwD8/f3Zv3+/sv8bb7xx37YGDBgAgIuLC0lJSQAcOHBA+RLw8fGhbt26T+MypH8p\nGdglqQIqlarc6zp16jxwfwMDg/u+V7NmTQC0tbUpKip68s5J1drWrVs5c+bME7UhA7skVeDy5ctE\nRUUBsGHDBlxdXUlKSuL8+fMArF27Fi8vr8du393dnZ9++gmA33//nfT09CfvtPRSKS4urnD7CxHY\nVSqVuUqlilCpVGdUKlWcSqV690nblKTnzdramm+++QZbW1vS09MJDAwkODiYQYMG4eDggJaWFuPG\njXvs9oOCgvj999+xt7dn06ZNNGzYECMjoyq8AulpWrBgAUuWLAEgMDCQLl26ALB3716GDh3Kxo0b\ncXBwwN7enmnTpinHGRoa8v777+Pk5ERUVBTTp0+ndevWODo68sEHH3Do0CF++eUXpk6dilqtJjEx\n8fE6KIR4oh+gEdDm/343As4BrR90jIuLi5Ckf7P8/HxRWFgohBDi0KFDwsnJ6Tn3SHoUUVFRwtfX\nVwghhIeHh2jbtq24ffu2mD17tpg9e7YwNzcXN27cEIWFhaJz585iy5YtQgghABEaGiqEEOLmzZvC\nyspKlJSUCCGESE9PF0II4e/vLzZt2lTheYFoUYm4/MR37EKIa0KI4//3exZwFmjypO1KUnV2+fJl\n2rZti5OTE5MnT2bFihVP7Vx3Tr+UqoaLiwsxMTHcunWLmjVr0qFDB6Kjo4mMjKROnTp4e3tjamqK\njo4OQ4cOVR60a2trM3DgQACMjY3R09Pj7bff5ueff0ZfX7/K+lelC5RUKpUF4Awcqcp2Jam6sbS0\n5MSJE0/9PPcbx5WejK6uLi1atCAkJISOHTvi6OhIREQE58+fx8LCgpiYmAqP09PTQ1tbGwAdHR2O\nHj3Knj17CAsL4+uvv2bv3r1V0r8qe3iqUqkMgc3AFCHErQreH6NSqaJVKlV0ampqVZ1WkqqNpKQk\nbGxsKr0wqrILpsrs3buX119/XXn9xx9/0L9//2d7kdWIp6cnCxcupFOnTnh6erJs2TKcnZ1p164d\nf/75Jzdv3qS4uJiNGzdW+KA9OzubzMxMXn31VRYvXszJkycBMDIyIisr64n6ViWBXaVS6VIa1NcL\nIX6uaB8hxHIhhKsQwtXU1LQqTitJ1U5lF0bl5+c/8oKpzp07Ex8fT9mNVXBwMCNHjnzm11hdeHp6\ncu3aNTp06ICZmRl6enp4enrSqFEjPvvsMzp37oyTkxMuLi7069fvnuOzsrLo06cPjo6OeHh4sGjR\nIgD8/PxYsGABzs7Oz/XhqQr4AfhvZY+RD08l6V4XL14U5ubmyus9e/YIb29v4enpqWwLDw8X/fv3\nFxqNpsLtQgjRvHlzkZSUpLx358O4+fPni0WLFon09HRhYWGhPMCVXg5U8uFpVYyxuwNvAadUKpXm\n/7Z9JITYWQVtS9K/SkULo9LS0h65nfstmBoxYgSvvfYaenp6DBo0CB0dmQewOqqKWTEHhBAqIYSj\nEEL9fz8yqEvSY6jswihra+vHWjDVuHFjGjduzPz58xkxYsTTuxDpuZIrTyXpBVLZhVF6enqPvWBq\n6NChmJubY2tr+5SvRnpeVKXDNs+Wq6uriI6OfubnlaQXWVJSEn369OH06dNP9TyTJk3C2dmZt99+\n+6meR6p6KpUqRgjh+rD95ACbJP2LuLi4YGBgwJdffvm8uyI9RTKwS9ILwsLC4qnfrd9v4YxUvcgx\ndkmSpGpGBnZJkqRqRgZ2SZKkakYGdkmSpGpGBnZJkqRqRgZ2SZKkKlQVpe2elAzskiRJVUgGdkmS\npBfAw2qYjh8/HldXV+zs7AgKClKOq0zN0sTERHx8fHBxccHT05P4+Pinfj1ygZIkSf96np6efPnl\nl0yePJno6GgKCgooLCwkMjKSTp06MWjQIOrVq0dxcTFdu3YlNjaWJk2asGXLFuLj41GpVGRkZFCn\nTh369u1Lnz598PX1BaBr164sW7YMS0tLjhw5woQJE6qsUtL9yDt2SZKemcjISOzs7FCr1eTl5d13\nP29vb55lPqkH1TD19PTkp59+ok2bNjg7OxMXF8eZM2cqVbM0OzubQ4cOMWjQINRqNWPHjuXatWtP\n/XrkHbskSc/M+vXrmTFjBsOGDXveXSnnQTVMa9WqxcKFCzl27Bh169YlICCA/Pz8StUsLSkpoU6d\nOmg0mvuc+emQd+ySJD2W119/HRcXF+zs7Fi+fDmbNm3ivffeA+Crr76iZcuWAFy4cAF3d3dWrlzJ\nTz/9xMyZMxk6dCj79u2jT58+SnuTJk0iJCTkeVwKcP8aprdu3cLAwABjY2OuX7/Orl27gMrVLK1d\nuzYtWrRQas8KIZT9niYZ2CVJeiyrV68mJiaG6OholixZQseOHYmMjARKh1zq16/P1atXlXHqUaNG\n0bdvXxYsWMD69eufc+/vdb8apk5OTjg7O2NjY8OQIUNwd3cHKl+zdP369axatQonJyfs7OzYtm3b\nU78WORQjSdJjWbJkCVu2bAEgOTmZ5ORksrOzycrKIjk5mSFDhrB//34iIyMZMGDAc+7tw3Xt2pXC\nwkLl9blz55Tf7/eXxNGjR+/Z5u7ufs90x99++61qOllJ8o5dkqRHtm/fPsLDw4mKiuLkyZM4OzuT\nn59Px44dCQ4OxtraGk9PTyIjI4mKilLucu+ko6NDSUmJ8jo/P/9ZXkK1JgO7JEmPLDMzk7p166Kv\nr098fDyHDx8Gyo9TOzs7ExERQc2aNTE2Nr6njebNm3PmzBkKCgrIyMhgz549z/oyqi0Z2CVJemQ+\nPj4UFRVha2vL9OnTcXNzA0oDe3JyMp06dUJbWxtzc3M8PDwqbMPc3JzBgwdjb2/P4MGDcXZ2fuR+\nZGRk8O233z7RtQCMGjVKGT4xNDSscJ+AgADCwsKe+FzPgqx5KknSS+tR68QKIRBCoKX1v3va4uJi\ntLW1ldeGhoZkZ2ffc2xAQEC5hUfPQ2Vrnso7dkmSXlrTp08nMTERtVrN1KlTWbBgAW3btsXR0VFZ\n+p+UlIS1tTXDhw/H3t6e5ORkDA0Nef/993FyciIqKuqeBVGBgYHY2dnRtWtXUlNT7zlvTEwMXl5e\nuLi40LNnz2ey6OhRyMAuSdJL67PPPqNVq1ZoNBq6d+9OQkICR48eRaPREBMTw/79+wFISEhgwoQJ\nxMXF0bx5c3Jycmjfvj0nT568Z6goJycHV1dX4uLi8PLyYs6cOeXeLyws5J133iEsLIyYmBhGjhzJ\nxx9//MyuuTLkdEdJkqqF33//nd9//10Zq8/OziYhIYFmzZrRvHlz5TkAgLa2NgMHDqywHS0tLd54\n4w0Ahg0bds9Uzb/++ovTp0/TvXt3oHQop1GjRk/jkh6bDOySJFULQghmzJjB2LFjy21PSkrCwMCg\n3DY9Pb1y4+oPolKp7jmPnZ0dUVFRT9bhp0gOxUjSc7JkyRJsbW0ZOnRohe9HR0czefJkoHSBzKRJ\nk55l914Kdy7f79mzJ6tXr1YefF69epUbN248cpslJSXK7JcNGzbcM1RjbW1NamqqEtgLCwuJi4t7\nksuocvKOXZKek2+//Zbw8HCaNm1a4fuurq64uj50AsS/Wv369XF3d8fe3p5evXoxZMgQOnToAJTO\nblm3bl2l78zLGBgYcPToUebPn0+DBg0IDQ0t936NGjUICwtj8uTJZGZmUlRUxJQpU7Czs6uy63pS\ncrqjJD0H48aNY/Xq1VhbWzNs2DC2bt1Kfn4+tWrVUlZu7tu3j4ULF7Jjxw5CQkKIjo7m66+/ZtOm\nTcyZMwdtbW2MjY2VB4RS9VfZ6Y7yjl2SnoNly5bx22+/ERERQY0aNXj//ffR0dEhPDycjz76iM2b\nN9/32Llz57J7926aNGlCRkbGM+z1yyk6OpoffvhBqZD0byADuyQ9Z5mZmfj7+5OQkIBKpSqXiKoi\n7u7uBAQEMHjw4Jciudbz9m8c0pIPTyXpOZs5cyadO3fm9OnTbN++/aHJsJYtW8b8+fNJTk7GxcWF\ntLS0Z9TTJ3d3DncoHQv/+OOPcXJyws3NjevXrwOlKz0nT55Mx44dadmypfJAUwjB1KlTsbe3x8HB\nQRkDHz58OFu3blXONXToULZt21Yu7/vs2bMZOXIk3t7etGzZstxd/Lx587C2tsbDw4M333yThQsX\nPpPP5GmoksCuUqlWq1SqGyqVqnLreiVJUmRmZtKkSRPg/ulh75SYmEj79u2ZO3cupqamJCcnP+Ue\nVp27c7inpaWRk5ODm5sbJ0+epFOnTqxYsULZ/9q1axw4cIAdO3Ywffp0AH7++Wc0Gg0nT54kPDyc\nqVOncu3aNd5++23l88vMzOTQoUP07t37nj7Ex8eze/dujh49ypw5cygsLOTYsWNs3ryZkydPsmvX\nrmdalu9pqKo79hDAp4rakqQX2uzZs1m4cCGzZs0iPDz8idv78MMPmTFjBs7OzhQVFSnbQ0JCuHDh\nwj37T506FQcHB1q0aKGspHzQdMiQkBBSUlKeuJ8PU5nzLFmyRLkzT05OJiEhgRo1aih31C4uLiQl\nJSn7v/7662hpadG6dWvlTv7AgQO8+eabaGtrY2ZmhpeXF8eOHcPLy4uEhARSU1PZuHEjAwcOREfn\n3tHm3r17U7NmTUxMTGjQoAHXr1/n4MGD9OvXDz09PYyMjHjttdeq7oN5DqpkjF0IsV+lUllURVuS\n9LKYO3fuEx1fFsBMTEzKFXWYP38+ABYWFtjb2wOlwxIBAQFA6R0roMyaqVu37gPPExISgr29PY0b\nN36i/j7Mw85zZw53fX19vL29yc/PR1dXV1kEpK2tXe7LrWbNmsrvlZnBN3z4cNatW8ePP/5IcHBw\nhfvc2ebd56su5Bi7JFXCJ598gpWVFR4eHvz1119A+TSu06dPp3Xr1jg6OvLBBx8AsH37dtq3b4+z\nszPdunVT7jhnz57NW2+9RYcOHbC0tFSGHvbt24eFhQUGBgbUqlWLdevWUVJSgre3N5988gkODg7Y\n2tpiZGSkBPy9e/dy8+ZNbt68yYYNGxg9ejR2dnb06NGDvLw8wsLCiI6OZujQoajVavLy8rCwsGDG\njBmo1WpcXV1ZvHgxVlZWtGrVig4dOijXdGdCLX9/f/r06UNSUhK2traVOs/d7pfD/VF5enoSGhpK\ncXExqamp7N+/n3bt2in/Jv/9738BaN26daXbdHd3V55vZGdns2PHjsfq24vimQV2lUo1RqVSRatU\nquiKsqVJ0osqJiaGH3/8EY1Gw86dOzl27Fi599PS0tiyZQtxcXHExsbyn//8BwAPDw8OHz7MiRMn\n8PPz44svvlCOiY2NZe/evURFRTF37lxSUlL466+/uHz5MkePHiUlJYWUlBROnz5NQUEBS5cuZe/e\nvURERHD79m1u3bp1Tz8zMjKYOHEicXFx1KlTh82bN+Pr64urqyvr169Ho9Ggq6sLQLNmzdBoNLi7\nuxMcHExMTAyHDx9Go9EApXlX7kyode7cOeUhbUJCwkPPU6tWrXv6d78c7o+qf//+ODo64uTkRJcu\nXfjiiy9o2LAhAGZmZtja2jJixIhHarNt27b07dsXR0dHevXqhYODQ4XFQV4Wz2y6oxBiObAcShco\nPavzStKTioyMpH///ujr6wPQt2/fcu8bGxujp6fH22+/TZ8+fZTx4itXrvDGG29w7do1bt++TYsW\nLZRj+vXrR61atahVqxadO3fm6NGjnDp1CnNzc2UFo4eHBxcvXiQrKwsXFxdMTU25efMm+vr6ZGdn\ns2DBAvLz8/H19aVHjx4YGBgwbtw4cnNzKS4uxtLSEgCNRsOXX35JQkICb775Jjdv3uTQoUOEhIRg\nbGxM3bp1mTFjBl9//TXa2tr8+uuv7Nq1i7S0NMLDw6lduzapqanUr18fKB0iWrJkCadPn+bKlSvo\n6uoybNiwh36ONWvWZNeuXfdsvzP3ua+vr5Lv/O4HyWX7qVQqFixYwIIFC+5pKzc3V7nOMt7e3nh7\newOlfy3d6c487h988AGzZ88mNzeXTp064eLi8tBrelHJoRhJekI6OjocPXoUX19fduzYgY9P6TyC\nd955h0mTJnHq1Cm+//77ctMY704sdffrO7dra2sr48tlbWRkZNC/f3/09PQwNjbm1KlT5OXl8fnn\nnxMbG0vjxo3LlZorLCwkOjqa999/H4Dr169z6NAhhgwZUu4BoxCCTZs2MWTIEGbNmsXt27c5fPgw\n69evp1mzZkDpkEqXLl04evQo48eP57fffiMnJ4f8/HwlK+KTKi4ufuRjwsPDsbW15Z133nmsu+0x\nY8agVqtp06YNAwcOpE2bNo/cxouiqqY7bgSiAGuVSnVFpVK9XRXtStKLoFOnTmzdupW8vDyysrLY\nvn17ufezs7PJzMzk1VdfZfHixZw8eRIoP41xzZo15Y7Ztm0b//nPf5gzZw7btm0jLy8PR0dHkpOT\nOXv2LJmZmRw4cAALCwvs7Ow4cuQIN2/e5KeffiIvLw8zMzNeeeUVAJycnEhLS0MIgZeXF1C6KOfS\npUtA6QPCuxNZ9e3bt1wOle3btyurWA0NDfHx8WHHjh00b96c+Ph4UlNTuXjxItevXyc7O5vPPvuM\n+vXrs3TpUoqKirh8+TIGBgaUlJSwYMECZX54YGAgXbp0AUqfBwwdOpSNGzfi4OCAvb0906ZNU/pw\nd/GLip5bpKamMnDgQNq2bUvbtm05ePCgcny3bt24dOkSU6ZMeeR/YyhN+KXRaIiPj2fGjBmP1caL\noqpmxbz58L0k6eXUpk0b3njjDZycnGjQoAFt27Yt935WVhb9+vUjPz8fIQSLFi0CSv/sHzRoEHXr\n1qVLly5cvHhROcbR0ZE1a9aQn5/PwoUL8fPzY9++fTRr1gxXV1dKSkpo2rQp9vb29O3blwMHDmBu\nbo6RkRG6urrKsAiUBu6KHlaWadiwIYsXLyYkJETJSFg2rFTmtddeo06dOkDpXfuaNWtISkoiLS2N\ngQMHkpWVRVpaGoMGDaKwsJCwsDBsbGyYMmUKGzduxM/PDwsLC1JSUlixYgX29vZMnjyZ6OhoCgoK\nKCwsJDIyEisrK6ZNm0ZMTAx169alR48ebN26lddff10pfvHll1+SlpbG22+/TXx8PCqVSvnSeffd\ndwkMDMTDw4PLly/Ts2dPzp49+wT/utVUWQ3AZ/nj4uIiJOnfaP78+aJevXrCwsJC+Pn5iQULFgh/\nf3+xadMmERERIVq1aiVsbW2Fg4ODeP/994UQQvzyyy+iXbt2Qq1Wi65du4qjR48KOzs7ERQUJIYN\nGyaaNWsm6tWrJ5o0aSL2798vIiIiRLNmzYSFhYWwsrISjRo1EkeOHBFCCLFhwwZRp04dYW5uLj78\n8EMhhBDBwcHCyMhIpKamCh8fHwGIPn36iFdeeUVoaWmJOXPmiA8//FBoa2sLKysroaenJ3r27Cm8\nvLxEq1atxIoVK4SBgYFo3769qFmzpmjXrp1o1qyZyMzMFB06dBBmZmaiZcuWwsLCQtSoUUO89dZb\nyuexcuVKERgYKIQQQltbWxQVFQkhhCgsLBSOjo5ixIgRYvPmzaKgoEAIIYSpqalwcnJSfho3biyy\nsrKe2b/f8wZEi0rEWJkrRpKekbLZNePGjaNWrVqEhISUe0CXmZnJ33//TVZWVrm71LLZNSqVipUr\nV/L9998rx8TGxjJu3DjS09P54YcfePfdd8nIyODKlStER0fj6OiIiYkJERERNG3alGnTptGzZ0/6\n9+/P999/X24J/p0SExNRqVS0bt2abdu2KQ+Mi4uLqVu3Lunp6cqMna1bt5KTk8OQIUPIzs7Gy8uL\nsLAwQkJCuHnzJn5+fjRo0IAvv/yyXBHpu91Z/KLsucWePXsICwvj66+/Zu/evZSUlHD48GH09PTK\nHXt3Qep/vcpE/6r+kXfs0r/R4sWLxcyZM5XXgYGB5e7Y73eXGhsbK7p37y7s7e2FlZWV6NmzpxBC\niKCgoHLtvfXWW2LLli0iIiJCeHp6KttXrVol3n33XbF169b73i03b95cpKamisjISKGjo6PsM2rU\nKGFraysaNGgg3NzcxLFjx0RQUJAICgoS7u7uwszMTAghRI0aNYRGoxF2dnbixx9/FM7OzsLc3FwY\nGRmJK1euCHNzc9G7d2+hr68vGjduLFq1aiX8/PyEvr6+6NChg8jJyRF6enpCrVYLe3t7MWzYMHH5\n8mUhhBBbt24V2trawt7eXrRs2VJ8+umnSp/9/f2Fs7Oz2LhxY5X+W72oqOQdu5wVI1W5sspAdevW\n5bPPPgP+twxfur+7Z9eYm5vj6OhI//79Kz275siRIyQlJVV61k1FioqKlLH42NhYpRDIjRs3+Oef\nf5QFTDo6OhgaGnLgwAF0dXXZsGEDULoit0aNGly7dg1dXV3MzMzQ09OjY8eOqFQqpk2bRmJiIkeO\nHGHSpEnY2NiwaNEiCgoKCA0NVWb4eHh44ODggK+vL5988gmnTp3CxcWFn376CUdHR1JSUoiPj+f4\n8eP4+fk9+gdejcnALlW5b7/9lj/++IP09HQlcZP0aLNrpk+fzs2bN4mNjcXIyOiBs2vy8/NJS0tj\n3759JCYmcunSJY4ePcrFixcpKSkhNDQUDw8P2rVrx59//snNmzcpLi5m48aNyiyaO9WoUYNvvvkG\nW1tbcnNzadu2LaNHj+bixYtMnjy53MPjoKAgJk6cSF5eXrkl/yYmJhQWFuLm5sbmzZs5d+4cJiYm\nQOmMHHNzcy5cuMDnn3/OsGHD2LNnDx4eHlhZWQEwfvx4XFxcWLduHR06dFBmz4wdO5YWLVooUzrv\nrm4klZKBXapS48aN48KFC/Tq1YvFixdXmJjK29ubwMBAXF1dsbW15dixYwwYMABLS0tl1WZ1k5SU\nxPDhw5XZNY6Ojujp6fHdd98RExPD9OnTcXR0xMvLC0dHRxwdHdHW1katVjNo0CD69euHgYEBO3bs\n4PTp06SnpwPw999/06JFC5o0aUKbNm0IDw/nu+++Q1tbm5EjR2JoaIihoSH9+/dHV1eX3NxcOnfu\nTPPmzbl8+TLfffcdlpaWSntNmzbFwMCA+Ph4tLS0aNiwIbq6ujRo0AAtLS0uXrzIpk2bmDZtGunp\n6XzxxRdoa2ujUqmYNWtWuQU/q1evxsTEhEWLFuHo6FiueMjdf0GUzch5VHcXqZZKycAuVally5bR\nuHFjIiIiHpicqkaNGkRHRzNu3Dj69evHN998w+nTpwkJCXmp8os/qo8//phz584xceJEBg4ciLm5\nOT169OD8+fN8/fXXNGnShNjYWOLj47GyskKj0fDRRx9Rr149du7cSWpqKhMmTGDOnDlAaWDr378/\n+fn5bN26lb59+zJ+/Hi8vb2JiIigXbt2zJgxQ3loaWBgwKlTp5g/fz5FRUXK0EedOnWUPDLnz58n\nOjqa2NhYCgsLGTBgAJMnT6Zp06ZcvXqV7OxscnJyMDY25vDhwxw/fpxPPvlEmeZpamrK7NmzGTx4\nMPv37ycyMpLY2FguXryo3O1fvnxZGe7ZsGEDrq6uJCUlcf78eQDWrl2Ll5cX1tbWFW6XHkwGdum5\nKJtl4eDggJ2dHY0aNaJmzZq0bNnypcovXhXKqiDdnbK2TGZmJhkZGUpA8/f3L1fn9HFXe3bt2lVJ\nh9C6dWtlQdNPP/1EmzZtcHZ2Ji4ujjNnztxz7OHDhzlz5gzu7u6o1WrWrFmjHF/G0NAQe3t7rKys\nsLa25tq1a0rCM2tra2W4Jz09ncDAQIKDgxk0aBAODg5oaWkxbtw49PT0KtwuPZic7ig9F2WpU7W0\ntMqlUdXS0nqh06jGx8czcuRIsrKyqFevHps3b1bGjh9ER0eHkpIS5fWdD0DLrv9RU8jOnj2bffv2\n3TMcYWdnR1BQ0D3nvbsyU0Xpay9evMjChQs5duwYdevWJSAgoMKKTkIIunfvzsaNGx/Yx//85z98\n+umn2NjY0Lx5c1555RWSkpLQ0dFh3bp15fbt2rUrJ06cuKeN+22v6EtQKiXv2CXpEa1bt45Tp07R\nsWNHli35hD8HAAAgAElEQVRbVqljzMzMuHHjBmlpaRQUFDxSWtiyRF2RkZHAg4cjjIyMyMrKUl5b\nWFgQExMDoMxmeZBbt25hYGCAsbEx169fL5e068623dzcOHjwoDJEkpOTUy6nfJn27duTnJzMhg0b\nyiXmkp4ueccuSY/AxsZG+b2goKDc0v4H0dXVZdasWbRr144mTZqUa6cy1qxZo2RubNmy5X2LSPj5\n+TF69GiWLFlCWFgYH3zwAYMHD2b58uUVlom7m5OTE87OztjY2GBubo67u7vy3pgxY/Dx8VGeoYSE\nhPDmm29SUFAAlBYIKZvVcqfBgwej0WiUZy4WFhblHrJKVU915xSlZ8XV1VW87DUFpX+33bt3M2XK\nFKKioh57Rse/RZ8+fQgMDKRr167PuysvPZVKFSOEcH3YfnIoRqpyy5Yt44cffqjSNr29vSssMPyg\nWp9PS0lJCW+//Ta//PKLDOoPkJGRgZWVFbVq1ZJB/RmTQzFSlavusxZSUlIwNjZWCllIFatTp06F\n4+7S0yfv2KVKWbduHe3atUOtVjN27FiKi4sxNDTk448/VqrO31nTsyx9gEajwc3NTVkan56eTmJi\nYrkiBgkJCcrruXPn0rZtW+zt7RkzZky51Yxr165FrVZjb2/P0aNH7+njg3J1P0hISAgpKSmV/izq\n1q3Ll19+Wen9JelZk4FdeqizZ88SGhrKwYMH0Wg0aGtrs379enJycnBzc+PkyZN06tRJmaN8p+HD\nhytVfRwcHJgzZw6tWrXC2NhYqa8ZHBys1KicNGkSx44d4/Tp0+Tl5ZWbPZKbm4tGo+Hbb79l5MiR\n95yrLFf3sWPH2Lx5M6NGjarU9T1qYM/MzGTlypWV3l+SnjUZ2KWH2rNnDzExMbRt2xa1Ws2ePXu4\ncOECNWrUUOp7VrS45kELa0aNGkVwcDDFxcWEhoYyZMgQACIiImjfvj0ODg7s3buXuLg4pb2y6XKd\nOnXi1q1bSlrbMuHh4UyaNInWrVtjZWXF5cuXsbW1pUePHuTl5VX410NYWBjR0dEMHToUtVr9wIIV\nZRo3blypqYOS9LzIwC49lBACf39/NBoNGo2Gv/76i9mzZ6Orq6vk/HjUxTUDBw5k165d7NixAxcX\nF+rXr09+fj4TJkwgLCyMU6dOMXr06EeqE1qWq3vnzp0UFRVx8OBBzp49S506ddi8eXOFfz34+vri\n6urK+vXr0Wg01KpV6wk+KUl6McjALj1U165dCQsL48aNGwD8888/9ywfr8iDFtbo6enRs2dPxo8f\nrwzDlAVxExMTsrOz77krLsvkd+DAAYyNje8pWNyjRw+WLl0KQIsWLZTtLi4uJCYmPnBZviRVJzKw\nSw/VunVr5s+fT48ePXB0dKR79+5cu3btgceU3U2vWbOGqVOn4ujoiEajYdasWco+ffv2JTc3lx49\nepCSksKoUaMYPXo09vb29OzZ857aonp6ejg7OzNu3DhWrVp1zzmXLFlCdHQ0Pj4+JCcnK6tCtbW1\n7xm2kaRqrTLVOKr6R1ZQqt4mTZokVq9e/dD9ZsyYIUxNTav8/BcvXhR2dnbK6wULFoigoCDh6Ogo\n9u/fL4QorT40ZcoUIYQQffr0EXv37r2nnYiICNG7d+8q758kPS5kzVPpeZg5cyZHjhxh9uzZD9yv\nf//+REREUFBQgFqtxtLSkrNnzyqpe8vqaCYkJPDBBx9w+/Zt1q5dS82aNdm5cyf16tUjMTGRiRMn\nkpqair6+PitWrHjgUv37LcsPCAhQ6pBGRUXJcXbppScDu1Sl5s2bx7x58x6635YtW0hKSqJPnz5o\nNBrl9zKnT5/mxIkT5Ofn88orr9C/f3+Ki4tJTEykR48ehIWF4erqipmZGU2aNGHy5MlMmDCBZs2a\nUatWLQwNDWnZsiWrV6/mzJkzREVFkZSUxOHDhwH4/fffefXVVykoKKBVq1bExMRgaGjIb7/9xpQp\nU9DX18fDwwMofShrbW3NoUOHMDU1paSkBCsrK6KiojA1NX06H6QkPQE5xi69kDp37oyRkRGmpqbo\n6+uzf/9+9u7dy+eff46rqyvjx48nJycHPT09zp07x/Dhw5Vx//T0dKKioli8eDF9+/YlMDCQuLg4\nTp06hUaj4ebNm8yfP5/w8HCOHz+Oq6srixYtIj8/n9GjR7N9+3ZiYmL4+++/gdJUwsOGDWP9+vVA\n6bRKJycnGdSlF5YM7NIL6c5c4QUFBfTr1w8TExO0tLSUos/16tVDo9Fw4cIFdHV1OXv2LACvvfYa\nKpUKBwcHzMzMlAINdnZ2yl17RUUi4uPjadGiBZaWlqhUKoYNG6b0YeTIkUr+m9WrVyszeSTpRSQD\nu/Tc3J07/FGoVCosLCzYtGmTsu3kyZPAw4t4iP8rElE2L//MmTMVzrK5k7m5OWZmZuzdu5ejR4/S\nq1evx+q3JD0LMrBLz039+vVxd3fH3t6eqVOn3nc/PT09fvnlF6UWan5+Ph07dmTw4MGsWrWKli1b\nUlBQwLZt2yp13vsVibCxsSEpKYnExESAe6oDjRo1imHDhjFo0CC0tbUf55Il6ZmQgf0+lixZgq2t\nLUOHDn3kYz/99NPHPu/deUtGjRpVYc3J6mLDhg2cPn2aTZs2KcUXAgIC+Prrr5V9UlJSmDVrFl5e\nXixevJiioiKWLl3Kjh07SElJwcrKiri4uHJz5B/E1NRUKRLh6OhIhw4diI+PR09PTylI0aZNGxo0\naFDuuL59+5KdnS2HYaQXniy0cR82NjaEh4fTtGnTRz7W0NCQ7Ozsxzqvt7c3CxcuxNX1obn0pWcs\nOjqawMBAZSWtJD1rstDGExg3bhwXLlygV69efP7553To0AFnZ2c6duzIX3/9BZTeWQ8YMAAfHx8s\nLS358MMPAZg+fTp5eXmo1Wrlbv/111/HxcUFOzs7li9fDkBxcTEBAQHY29vj4ODA4sWLK0xIdWeB\nid9++402bdrg5OQkCxc8Y5999hkDBw7k//2///e8uyJJD1eZVUxV/fMyrDxt3ry5SE1NFZmZmaKw\nsFAIIcQff/whBgwYIIQQIjg4WLRo0UJkZGSIvLw80axZM3H58mUhhBAGBgbl2kpLSxNCCJGbmyvs\n7OzEzZs3RXR0tOjWrZuyT3p6uhBCCC8vL3Hs2DFle9nrGzduiKZNmwpnZ+dybUqS9O+BXHlaNTIz\nM/H39ychIQGVSkVhYaHyXteuXZVEVK1bt+bSpUuYm5vf08aSJUvYsmULAMnJySQkJGBtbc2FCxd4\n55136N27Nz169HhgPw4fPkynTp2UudT16tWrqkuUJKmaqZKhGJVK5aNSqf5SqVTnVSrV9Kpo80Ux\nc+ZMOnfuzOnTp9m+fXu5NLJ3TqU7ceIEI0eOxM7OTklfa2hoyBtvvMEXX3xBvXr1WLFiBSqViv79\n+xMZGcnJkydp3bo1Q4cOxdTUlDZt2nDr1i0AZs2ahVqtJjo6ml69evHVV18pbQLs27cPb29vfH19\nsbGxYejQoUq1oZ07d2JjY4OLiwuTJ08ut6JTkqTq74kDu0ql0ga+AXoBrYE3VSpV6ydt90WRmZlJ\nkyZNgNJx9ftxcnJi+fLlREdHU1hYyN9//01OTg7W1tZ07dqVOnXqEBgYSH5+PvPmzeOjjz6ipKQE\nf39/fv/9d8zNzQkNDeXChQtkZWUxd+5cNBoNarUaY2Njpk+fzv79+ykpKQHg1q1bnDhxgv/+97+c\nOXOGCxcucPDgQfLz8xk7diy7du0iJiaG1NTUZ/ExPdCdpfIqkpqaSvv27XF2diYyMpJXX331odkY\nZ82aRXh4OAD//e9/yc3NfWg/7lcQW5Kqm6q4Y28HnBdCXBBC3AZ+BPpVQbsvhA8//JAZM2bg7Oz8\nwEISFy9e5O2338bNzQ0dHR3c3NzQ0tLio48+oqioiMjISNLT03Fzc6Nly5ZcunQJb29v2rdvT/fu\n3cnIyGDQoEHk5eUxbtw41Go1ubm5nD17liFDhtCtWzeWL19OQUEBTk5OzJ07l3bt2tG0aVO0tLRQ\nq9UkJSURHx9Py5YtlXzkZVWHXmR79uzBwcGBEydO4Onpyc6dO6lTp84Dj5k7dy7dunUDKh/YJelf\nozID8Q/6AXyBlXe8fgv4uoL9xgDRQHSzZs2e9jOGZyoiIkK4u7uLnJwcIUTpA8+IiIhyD1GDgoLE\nggULlNdl7wUFBYn3339fFBcXi8LCQqGtra3sM2vWLDF27Nhy5yo77u6UshMnThTBwcHixIkTolOn\nTsr2bdu2PZfUs/PnzxeWlpbC3d1d+Pn5iQULFojz58+Lnj17ijZt2ggPDw9x9uxZceLECWFubi5M\nTEyEk5OTyM3NVR5cX7x4UdjY2IhRo0aJ1q1bi+7du4vc3FwhhBD+/v5i06ZN4quvvhK6urrC3t5e\neHt7CyGE2L17t3BzcxPOzs7C19dXZGVlCSH+9yB61apV4t1331X6unz5ciWFryS9yKjkw9NnNt1R\nCLFcCOEqhHCtbsmTMjMzqVu3Lvr6+sTHxysZBCt7bKNGjdDS0mLt2rUUFxcDsH37dsLDw1myZMkj\n9aXsoWxZ/dGyqkPPUkxMDD/++CMajYadO3dy7NgxAMaMGcPSpUuJiYlh4cKFTJgwAbVazdy5c3nj\njTcqLE2XkJDAxIkTiYuLU0rc3Wny5Mk0btyYiIgIIiIi7pvg606DBw9m+/btyoPw4ODgCotjS9LL\nqipmxVwF7pwK0vT/tv1r+Pj4sGzZMmxtbbG2tsbNza3Sx06YMIGBAwfyww8/4OPjg4GBAQCLFi3i\n6tWrtGvXDihd9Th37tyHtlerVi2+/fZbpa27qxA9C5GRkfTv3x99fX2gtO/5+fkcOnSIQYMGKfsV\nFBQ8tK0WLVqgVquBigtm3+3OBF8At2/fpkOHDuX2MTQ0pEuXLuzYsQNbW1sKCwtxcHB4lEuUpBda\nVQT2Y4ClSqVqQWlA9wOGVEG7L42aNWuya9eue7bfufr07sITZe9ZWloSGxurbP/8888BiIiIqPBc\nZcd5e3vj7e2tbL9zCX7nzp25cuUKWVlZTJw48ZFXsZblRi9b4l+RgIAA+vTpg6+v733bOHToEB07\ndgRKc5rXqVMHjUbzSH25c+aRtrY2eXl5D9xf/F+Cr7vzvNxt1KhRfPrpp9jY2MgUAVK188RDMUKI\nImASsBs4C/wkhIh70nZfdCEhIUyaNAl4+KyPB0lKSmLDhg1V2TVWrFhBXl4ednZ2ZGZmMnbs2Cpt\n/2E6derE7t27+fPPP8nKymL79u3o6+vTokULJRujEELJxliR5cuX061bN65cufLQ892ZJfLuBF/T\np09XZjOVZXIEaN++PcnJyWzYsOGleMAsSY+iSsbYhRA7hRBWQohWQohPqqLNl82BAwceK2lYZQN7\ndHQ0kydPBkrnsB86dOi++wYGBlKrVi3OnDnDunXrCAoKUlIXlI25+/n58euvvyrHBAQEEBYWRnFx\nMZ9++imJiYnUrVuXhg0b4uvrS05ODoMHD0ZfX5/atWvz66+/kp6eDpQuwGrdujWOjo74+flRr149\ncnJymDlzJg0bNsTCwgKA9evXs2rVKpycnLCzs3tgNsbg4GDWrl1bqVw9Y8aMwcfHh86dO9+T4Kus\njF5FBg8ejLu7O3Xr1n3oOco8yZe4JD0zlXnCWtU/zzKlwNq1a0Xbtm2Fk5OTGDNmjEhKShKvvPKK\nSE1NFcXFxcLDw0Ps3r1bCCHEmjVrhIODg3B0dBTDhg0TQghx48YNMWDAAOHq6ipcXV3FgQMHhBCl\nKQUmTpwohCid2WJqaiqSk5MrnPkhROksjnfeeUd06NBBtGjRQmzatEkIIUT79u1F7dq1hZOTk1i0\naFGF11CW0qDM3TNsKlI2eyYsLEx069ZNFBUVib///luYm5uLlJQU8fPPP4vhw4cLIYQoKCgQTZs2\nFbm5ueL7778X7733ngDE3r17hYuLi/D19RVvvfWWMDY2FteuXRNXr14V+vr6onPnzkIIIRo1aiTy\n8/OFEP9LjVCZPt7P2LFjlZkun332mXBzcxNqtVp06NBBxMfHCyFKP/9+/fqJbt26iebNm4ulS5eK\nL7/8UqjVatG+fXsl5ULZ7Bkh7p0V07t3bxEeHl7pWTEXL14Upqamj31dkvSkqOSsmGod2M+cOSP6\n9Okjbt++LYQQYvz48WLNmjVixYoVwtfXV3zxxRdizJgxQgghTp8+LSwtLUVqaqoQ4n+5WN58800R\nGRkphBDi0qVLwsbGRghRPrC7uLgIbW1tYW9vL1q1aiXUarVQq9XCwcFBtG/fXgghhImJiejevbso\nLi4WcXFxQk9PTxw7dkxs27ZNmJmZKfuePHlSCCHEu+++K4yNjUXHjh2Fn5+fMr3x4sWLwszMTDRu\n3Fg4OTmJ/fv3V/jlUxbYp0yZIlatWqV8JsOGDRPbtm0T8fHxon79+iI/P19s3bpV9OzZU7zzzjti\n4MCBonnz5kKlUgk9PT1hamoqPv/8c2Fubi709PSEk5OTcHJyErVr1xZNmzYVEydOFD179hQDBw4U\n8+fPF2FhYUKIJwvsQlQuV0+rVq3ErVu3xI0bN0Tt2rXFd999p1zz4sWLhRAVB/bk5GShq6urtNWh\nQwcRGxv70D7JwC49b5UN7NU6V8yePXuIiYlRZobk5eXRoEEDZs+ezaZNm1i2bJnyMG/v3r0MGjQI\nExMT4H+5WMLDw8vlQ79169Y9KXn79OnDhQsX2L59OzY2NlhbW6NSqcjJyeHixYsAWFhYYGhoiJaW\nFnXr1qWgoABXV1cGDBhA7dq1iY2NZe/evQwfPlzpU0FBAeHh4dSqVYt9+/Yp7YwbNw5DQ0M++OAD\nAIYMGUJgYCAeHh5cvnyZnj17PvSzuXbtGnp6euzevZvQ0FAmTJhA3759GThwIMOHD+eLL75QHlTu\n3bsXXV1dGjdurPRtwIABNGrUCIBff/2V/fv389lnn/Hll1/Sr9+jrU8bNWoU7733Hq1b37tg+UG5\nesrqohoZGWFsbMxrr72GoaEhS5YsITY2lnnz5rFlyxYOHz7M5s2bSU5O5q+//mLSpEkYGhpy5coV\njhw5QmFhIcXFxbi5uZGbm0urVq1YvXo1Q4cOZfLkyUybNg3gofl8JOlFUa0DuxACf3//e1Kt5ubm\nKg/lsrOzMTIyum8bJSUlHD58GD09vUqdr3bt2rRs2ZKEhAT09fWVdAQWFhbK/PaffvoJHZ3Sj/7U\nqVNK4rAuXbqQlpam5IsxMDBg1KhRHD9+nAYNGqCvr09MTAzBwcEUFxfzxx9/EBISQnh4OJGRkaSl\npSn5YrS0Sh+faDQaIiIi8Pf3559//mH9+vUsWLCAtm3bkpKSwoABA9DS0sLQ0JARI0bQunVrtmzZ\nghCCli1b0rhxYw4dOoSxsTF5eXmsXLmSJUuWcObMGYyMjKhZsyY6Ojo4Ojpy5swZMjMzsbOz4/r1\n67z77rvlPp9Zs2bRqVMnZcVomZUrVyq/z549W8mHA6W5ehwdHUlISCAxMbFc8Yu7y97dWRIvJSWF\nffv20bdvX3r06KFMFQ0KCmLVqlXo6ekxfPhwJk6cyMiRIxk+fDhLly7Fy8uLWbNmMWfOHHbu3Imj\noyNff/01nTp1emCVJ0l6kVTrfOxdu3YlLCyMGzduAPDPP/9w6dIlpk2bxtChQ5k7dy6jR48GSoPq\npk2blPJr//zzD1B6l7Z06VKlzQdN1yv7gqhTpw6nT5/ml19+KRekjYyMiI2NJTQ0VAnsWlpa9y3K\nkZaWxoQJEzh79iz6+vokJSXxzjvvMHjwYKZMmcLIkSP5+OOPlS+f3Nxc8vLyeP/995X0B82aNaNZ\ns2Y4OTnRpUsXatSoQXJyMjVr1qRHjx4YGRmhp6enlHqzt7enb9++qFQqrly5QnR0NHp6euzcuZMa\nNWowadIk4uLiqFWrFmq1mnbt2rFu3ToWL16MEIJ27dqxe/duGjRowJYtW1Cr1Uphirlz59KhQwd6\n9+6Nk5MT9vb2hIaGlsvh8umnnyqVkQYMGEBSUhIbN27k3Llz3L59GyidkfTVV1+xZcsWLC0tmTNn\nzj2f3bVr1zA1NWXXrl385z//oX79+hQXF5OdnY2Xlxft27dHR0eH2NhYevfuTUZGBl5eXgD4+/uz\nf/9+MjIyyMjIoFOnTgC89dZbNGjQQPlLSZJeVNX6jr1169bMnz+fHj16UFJSgq6uLosWLeLYsWMc\nPHgQbW1tNm/eTHBwMCNGjODjjz/Gy8sLbW1tnJ2dCQkJYcmSJUycOBFHR0eKioro1KkTy5Ytu+85\nnZ2dOXHiBE5OTqSkpJTLL9OxY0e++OILMjMzlTvq7t27s3PnTpycnPDw8MDExITatWsDYGxsrCy0\n6d69O59++ilXr14lKSmJkpISTE1NadSoET169GD+/PmcPn2ajIwM0tLSlLnZKpWK4cOHK/PNDQ0N\nOXjwIO7u7qSlpZGens57772nDLv4+fkhhCApKYm4uDhUKhUFBQWMGjWK7Oxshg4dys8//4yuri5n\nz56lsLAQlUqFv78/JiYm7Nq1i/79+3P58mUaNmyInp4eM2bMIDs7G7VajYmJCQYGBmRnZ6Orq8vM\nmTOVvDnu7u4UFhZSo0YNVCoVkZGR6OrqYmBggEqlUlblrlixgnPnzvHmm2+ydOlSZeXundLT06lZ\nsyZ9+vTh1VdfZfr06fcsiHr11VdZuXLlQ/PSSNLLplrfsQPKUvXY2FhiYmLw8vLi8OHDyh3qzz//\nrARBf39/Tp8+zcmTJ5W5zyYmJoSGhhIbG8uZM2eUoH5nXc7Zs2crwXj27Nnk5uaipaXF2LFjlXzt\nISEhfPLJJ/z4448MHjxYuUufO3cujo6OCCGIiYlhzZo1St9VKlW5a9HR0cHOzo59+/bRoEEDtLW1\nmTlzJkuWLOGHH37g+vXrFBcXY21traQX1tHRUTJClpSUKHe992NgYMClS5e4du0atWvXJj09HUND\nQ8aPH48QAnt7e4yMjJSKUr179y735bVnzx6mTZtGUVERPXr0wMfHh8aNG1NQUMA///xDs2bN2L17\nN1evXmXWrFnKF5larWbkyJGoVCqysrKwtLRk8ODB6OjoMGLECF555RWgdHqop6cn9erVY+XKlcr1\nfPzxx8rzkYCAANRqNadPn+b48ePMmzePK1eu4OnpiZmZmfIXxLZt23Bzc1OmPJZtX7t2LTVq1CA6\nOpo6depw4MABoHS65j///MMPP/zwwM9Qkp63ah/Yn5WkpCRMTEzo0KED586d48SJE8yfP7/cEngz\nMzOKiooICgpSttWrV4+tW7cSGxvL4cOHcXR0BGDKlClkZGQQFRUFwJkzZwgMDCQ1NZW0tDRiY2M5\nduwY9erVw8TEhFq1anHgwAFOnjypDPNA6dh+TEwMAL/88guFhYW4u7tz5MgRMjMzyc7OZseOHeWu\n5fjx42RmZlJSUkLLli0pKioiKioKXV1drl27RlFREU5OTty8eZP4+HjlOC0tLfLy8nBzc6NFixZM\nmzaN/fv34+Ligr29PefPn6dhw4YYGBhgbm7OokWLiImJISsrS/m8oHQI7datW9ja2mJoaEivXr3K\nfcnZ2NiQlZVFamoqGzduxNbWttw1l33W3bt3Jz8/nzp16vDaa6/RvXt31qxZQ2BgIDVr1iQnJ4fP\nPvsMgDVr1jB16lQcHR3RaDTs2rWLbt26ERwczMSJE1Gr1QghqFevHsOHD3/s/yeS9CzIwP4Cs7a2\n5ptvvsHW1pb09HTeeecdwsLCmDZtGk5OTqjVamWh0rx582jfvj3u7u7Y2NgobYwePZo///wTJycn\noqKilPwxgwYNIiYmBjMzM/T09JQcNVD6ELhp06YkJCTg6elJw4YNCQsLo7CwEB0dHZo1a8bGjRuJ\niYlR/hqA0hWnRUVFvPrqq+UKkmhra2Ntbc2VK1fYt28ftra2NG7cmICAAAwMDLC0tMTZ2ZlTp06h\no6Nzz0PRitIll5SU8P3337Nq1Sr++ecfZciqTM+ePfn77785fvw4u3fv5vz580remejoaAoKCrh6\n9SrGxsYUFxfzzTffkJWVRcOGDdm4cSOBgYGEhYXh4uJCZmYmvXr14vfff0dPT4/jx4/Ts2dPWrVq\n9cBhOUl6biozJ7Kqf16GmqfVXVkq25ycHOHi4iJiYmKU9+Li4sQrr7wirl+/LoQondOflJRULg3x\npk2bhL+/vxCi/Jx1R0dHERoaKuzs7ERQUJCYMmWKWLBggQgKChI2NjbCwMBANG/eXFmA1KRJE1G3\nbl3h7u4ubt++LXR0dMSCBQuEg4ODGDFihDAzMxNjx44VdnZ2ovS/a+kcdh8fH1GzZk1Rs2ZNMXv2\nbKVfd/axTZs2Qk9PT9SoUUOYm5uL8+fP3/M5XLx4UWhra4sTJ04IIYQYNGiQWLt2bbn5782bNxff\nfvutEKJ0jryDg4Myf75BgwZP9g8hSY8AOY9depAxY8Zw5swZ8vPz8ff3p02bNsp7FT10/uabbx7Y\nXtlQyZo1axgxYgTnz59Ho9EQHBzMqlWrAGjZsiU3btwgMTGRnTt3MnXqVGrXrk1xcTFxcXGo1Wp8\nfHyA/6XSzczMVKZw3vlXRatWrRBC8PrrrzNu3Dhl+50zjMqGoB6mMhkk+/btC4CDg4MyRbZsumdG\nRoZ8ACu9UGRg/5d6WH6aN954gzfeeKPctjuDpq+vrzLTJi0tjebNmwOgVqs5ceJEuePKpgcuXLiQ\nNm3aoK2tTefOnYmPj0cIwcSJE7G0tCQwMLDccQ9KElZUVERCQkKVJPCqTAbJO+fIV2aoSJKeJxnY\npScyc+ZMjhw5ck9a4rv179+fxMRE9u7dC5ROWVyzZg23b9/G2dn5kTJQNm3alF27dhEYGKjMOpIk\n6X/kw1PpicybN4+jR49Sv379B+63ZcsWYmNjlSmJgYGBShrd9evXK0U5KqNbt25cunSJKVOmPFJf\ntyLyEz0AABb+SURBVG7dWi49RGXl5+dz9OjRh+73yy+/KLNs7iclJeW+OewlqaqoysYvnyVXV1ch\nq8VLz9rDioNUpKioiPnz55fLzSNJz4tKpYoRQjy0co68Y5deauvWraNdu3ao1WrGjh1LcXExhoaG\nfPzxxzg5OeHm5sb169c5dOgQv/zyC1OnTkWtVpOYmEhiYiI+Pj64uLjg6empzMkPCAhg3LhxtG/f\nnsGDB7Ns2TIWL16spEfYvn077du3x9nZmW7dunH9+nWgdBFa//79OXToEAEBAUyePJmWLVtiampK\nWFgYULrewd7eXtl/wIAB+Pj4YGlpyYcffqhc16pVq7CysqJdu3aMHj1aKeoiSZUhA7v00jp79iyh\noaEcPHgQjUaDtrY269evJycnBzc3N06ePEmnTp1YsWIFHTt2pG/fvixYsACNRkOrVq0qLK5d5sqV\nKxw6dIiff/6ZcePGKUNHnp6eeHh4cPjwYU6cOIGfnx9ffPGFctzVq1eVtQXXrl3j/Pnz/Pnnn0yf\nPr3Ca9BoNISGhnLq1ClCQ0NJTk4mJSWFefPmcfjwYQ4ePFhuEZgkVYZ8eCq9tO6XlrlGjRr06dNH\nSSB2dxrjWbNm0bZtWw4d+v/t3X9UlVW+x/H31ugiKeKIhZV5SFTgAIfwIJCijT9I7rQsVMaKqTHT\nZjkyYz+u5uSqiGlmleO0Gqcab45hLtG4iECDOhpKwwxlgnZCpEAJL5VkaIooOnpg3z+Q5/JTMH4c\nPXxfa7FW55yHw/fB+PCwn72/+yPuuusubr31Vtzc3Jr1komNjTXaTkDD1fXmzZu5cOECQ4cO5dy5\nc5w8eZJvv/0WFxcXbDYb0dHRFBUV8c0333D+/HkWL15MYmIiAwcO5Pjx49hsNmMqaExMDNOmTWPq\n1Kncf//9hIWFUV1dzd13382iRYuYPHmy0To6NjaW0tLSXviOCmchV+ziuqUvt2W22WzYbDZKSkpI\nSEjAxcXFmFevlGo1HTExMZHJkyfj4eGB1Wpl06ZN2Gw2Pv/8c+OYpnPmoWEbvf379/PJJ5+wf/9+\nlixZQl1dHUlJSYwbN47U1FQ8PT0JCAjgqaeeYubMmVgslma1Pvroozz77LP4+PgQGBhIZmamMXXS\nbrczceJEFi9ebGxfKMQPJcEu2tR0LPhKXnjhBbKzswGatd81mUycOHECaOhq+UOtX7+eY8eOtfla\ne22Zm6qvr+ejjz7CbDazZ88eTp48ybx589i1axfe3t5UVVUBDcH705/+FKvVSkZGRrNwXb16NWvX\nrmXAgAGYzWZqa2vZuHEjkyZNYv369UZzuXXr1hktFrZs2WK0bP7qq6+ora3l9OnTAJSVlZGamsqH\nH35oNDGbNWsWAGPGjKG6upp//OMfnDp1CrvdTlpa2g/+/om+SYL9GlBVVWXcjGvsMNhZNpuN7du3\n91BlV1ZXV0diYmKrjTNautLG2x25UrA3XSEbFBTE9OnTqaysbHZMZWUlvr6+HDp0CB8fHxITE3n/\n/ff59ttvSU5OprKykoceegiz2cydd95JQUEBM2fOpLi4mMLCQqBh0dLx48cZPXo0GzduxMPDg337\n9rFjxw4+/vhj/Pz8OHjwIPX19cYvmfasXbuW4cOHk5WVhZeXl9G8rPHKvXH457nnnmP8+PFMmDAB\nk8kk8/XFVZFgdzC73c7u3bsJDAzk008/JTIy8qo+vyeD3W63ExcXh5+fH3PmzKG2thaTycSzzz5L\nSEgIqampzJs3z5jx0Z7GHZHOnj3L1KlTCQkJMYYioOGvAz8/PxYuXIjZbCYqKorz58+zZcsWCgoK\niIuLIzg4uM0VoS3bMoeHhzdbIXvnnXfy/vvvAw2NwRYtWsTMmTPx8vLC29uboKAgNm/eTHFxMSaT\niZCQEGw2G999950x510pxaRJkygsLGTYsGGcOXOGxx9/nBtvvBFfX1/27dvH999/T2JiIoMHD6am\npgZPT0+jDQE0NEgbMmQIPj4+DBs2jPj4eKKionjrrbeMY7KysoxmZg8//DCHDx8mLy+P77//Hqu1\nwxluQhgk2LvB0aNH8fX1bRWCjX+ijxs3jnvvvde4mrznnnt48sknsVqt/OlPf2LZsmVkZmYa4bVr\n1y4iIiIICQkhNjbWCKr8/HzuvvtuLBYL48ePp7q6mhdeeIGUlBSCg4O7fWy2pKTE2MHJ3d3dCKGh\nQ4dy4MABHnzwwat6P1dXV9LT0zlw4AA5OTk888wzRh+Yw4cPs3jxYg4dOoSHhwdpaWnMmTMHq9VK\ncnIyNpuNAQMGXPU5tGwX0N7y//LyclatWsXu3buNXZUaO1QOGDAArTV+fn4sX76cYcOG4e7uzvPP\nP09xcTEWi8Vov+Dl5UV6ejqVlZXGXyqNX/Pdd9/ls88+49SpU1RUVLBz5852Z7wkJCQQHBxMQEAA\n3t7ePPDAA1d97qLvklkx3aSkpIR169YxYcIE5s+fz5tvvkl6ejqZmZkMGzaMlJQUVqxYwTvvvAPA\nxYsXjfHooUOHUlBQwBtvvMGJEyd4+eWXyc7O5qabbuLVV1/ltddeY/ny5cydO5eUlBRCQ0M5c+YM\nbm5uJCYmGp/b3UaMGGFcQf7sZz9j9erVAK16yHSW1prnnnuO3Nxc+vXrxzfffGPMAe9MI66edObM\nGW666SYGDx7M8ePH2bFjB/fccw/QcMWekpLSbCMPs9nMfffdx6uvvkpaWho+Pj7GAqglS5Ywbdo0\nXFxcSEhIMHrgBAcHk5yczKhRo4CGfjtffPGFsVE5NGzs0tvnLpyPBHs3aRmCv//97ykqKmL69OlA\nw3j08OHDjePbC8e9e/dSXFxsvNfFixeJiIigpKSE4cOHG1P7Gnds6kktd3BqfNxyxkhnJScnU1VV\nxf79+3FxccFkMhlXxZ1pxNWTLBYLd911F76+vs3+La/E1dWVpKQkYmNjsdvthIaGGp0mX3zxRR5/\n/HGef/554xcEwOuvv05OTg79+vXDbDYTHR3dU6fUZWvWrMHNzU02FrkOSbB3k5YhOGjQIMxms7ED\nUkvthaPWmunTp7N58+Zmzx88eLB7Cr0KFRUVfPzxx0RERLBp0yYmTpzYqnPj1aiurubmm2/GxcWF\nnJycVjNY2jJo0CBqamqMx6tXr+Yvf/kLISEhJCcnX/FzTSYTRUVFxuO2WgI0vVpu3A6xpZZX0E2P\nmzp1apvfk8jIyDbnnjfdGP1a17Qdsri+yBh7N2kMQWhoiRseHk5VVZXx3KVLlzh06FCH7xMeHk5e\nXh5HjhwB4Ny5c5SWljJ27FgqKyvJz88HoKamBrvd3ir4ulPLHZwWLVrUpfeLi4ujoKCAwMBANmzY\n0Gynp/Y0Lu9vvP/w1ltv8cEHH3QY6n1N432eefPmMWbMGOLi4sjOzmbChAmMHj3auMH7wAMPEBQU\nRHh4OIWFhdTX12MymYypmACjR4/m+PHjJCQksGrVKoB22y+Ia1RnduPo7g9n20GpvLxcjx07VsfF\nxWlfX189a9Ysfe7cOf3pp5/qyMhIHRQUpP39/fXbb7+ttdZ68uTJOj8/3/j8pKQkvXjxYuPx7t27\ntdVq1YGBgTowMFBnZmZqrbXet2+fDgsL00FBQTosLEzX1NTokydPaqvVqi0Wi37vvfd698R72S9+\n8Qtj5yV3d3dj1yattTabzbq8vFyXl5drX19fvWDBAu3v76+nT5+ua2trHVh19ysvL9dms7nVc/37\n99eFhYW6rq5Oh4SE6Mcee0zX19frjIwMff/99+v4+Hhjt6ndu3dri8Witdb617/+tX7nnXe01lrv\n3btXT506VWvdfGesKVOm6NLSUuOYH//4x71yrqI5OrmDkgR7N2jrB030jJEjR+qqqqpmoaN182Bv\na6s7R2lZZ3doL9h9fHyMx4888ojeuHGj1lrrsrIybbFYdHBwsC4pKTGOuf3223V1dbXOy8vT9957\nr9a6Yeu/xguQxtpramq0q6urtlgsxoevr2+3npPonM4GuwzFCKfj6Bk2HemOHZfq6uqazfu/cOEC\nSiljuGTnzp3Gyt9ly5ZRUVFBaWlpm/3iIyIiOHLkCFVVVWRkZBirYBvV19fj4eFhtG5o2X5BXHsk\n2LtBy5t0oufdcMMNxvJ9wJhdA52fu95Tfve73zFmzBgmTpxISUkJ0HrtQnutfxMSEvj5z39OZGQk\nI0eOZOvWrSxbtozAwEBmzJjBpUuXgIbptR999BFKKY4cOcL27ds5duyY0a3SarWydu1ao6ZLly4x\nf/58vL29gYabxp6enri7u6OUIiYmhqeffho/P79Wm6a4u7vj7e1Namoq0PBX/pW2LRSOJ7NixHXJ\nZDKRlZUFwIEDBygvL3dwRQ3279/Pe++9h81mw263ExISwrhx44DmaxdOnTrF3r17UUrx17/+lZUr\nV/LHP/4RaLhRmZOTQ3FxMREREaSlpbFy5UpiYmLYtm0bwcHBmEwm42Z8SEgIubm51NbWEhsbCzTc\nzG+6oGvw4MG89NJLzJ8/n6CgINzc3Hj33XeN1+fOnUtoaGi7M4OSk5NZtGgRL7/8MpcuXeLBBx9s\n1uRMXFu6FOxKqVggAfADxmutZVsk0Stmz57Nhg0bMJvNhIWFMWbMGEeXBMA///lPYmJijK3+mrYV\naLp24euvv2bu3LlUVlZy8eJF40oaIDo6GhcXFwIDA6mrq2PGjBkABAYGcvToUYKDg7Hb7YSFhVFb\nW0tFRQVubm7ccsst2Gy2VjUNHDiQ119/nR/96EdkZGS0WbfVajVWATdquo+tt7c3f//736/+GyIc\noqtX7EXALOC/u6EWITrUdLx8165dbR7T0dx1R2m6duFXv/oVTz/9NDNnzuTDDz9sFqKNQ0n9+vVr\n1oK4X79+2O12/v3vf3Ps2DHy8vIYMWIEUVFRxi+H1NRUYmNj0VpTWFgoV9V9VJfG2LXWn2utS7qr\nGCGud5MmTSIjI4Pz589TU1PD3/72tzaPq66u5rbbbgNoNiTSGY0bgnh6enL27FmjC2VycjLr1q3D\nYrFgNpuNJmui7+m1MXal1BPAEwB33HFHb31ZIXpVSEgIc+fOxWKxcPPNNxstIFpKSEggNjaWIUOG\nMGXKlKu6RxAUFMTy5csJCAjAy8uL6OhoRo4c2e5wSXvj5sJ5qZbjaq0OUCob8GrjpRVa68zLx3wI\n/Fdnx9itVqtuvIkkxNU6ffo0mzZtarZHaUcaG3TNmTOnBysTomcppfZrrTvs4dzhUIzWeprWOqCN\nD/k7TzjE6dOnm/UxF0I0J/PYxXVn+fLllJWVERwczNKlS1m6dCkBAQEEBgYaPem11sTHxzN27Fim\nTZvWbGejxMREQkNDCQgI4IknnkBrTVlZGSEhIcYxhw8fbvZYiOtJl4JdKRWjlPoaiAC2KaV2dk9Z\nQrTvlVdeYdSoUdhsNsLDw7HZbHz22WdkZ2ezdOlSKisrSU9Pp6SkhOLiYjZs2NBse774+Hjy8/Mp\nKiri/PnzZGVlMWrUKAYPHmxMF0xKSuKxxx5z1CkK0SVdnRWTrrW+XWv9H1rrW7TW93ZXYUJ0xr/+\n9S8eeugh+vfvzy233MLkyZPJz88nNzfXeP7WW29lypQpxufk5OQQFhZGYGAge/bsMRb6LFiwgKSk\nJOrq6khJSeHhhx921GkJ0SUyFCP6lAsXLvDLX/6SLVu2cPDgQRYuXGi0I5g9ezY7duwgKyuLcePG\ntVpaL8T1QoJdXHea9qCPjIwkJSWFuro6qqqqyM3NZfz48UyaNMl4vrKykpycHOD/e8o0zgFvuhG3\nq6urseG1DMOI65n0ihHXnaFDhzJhwgQCAgKIjo4mKCgIi8WCUoqVK1fi5eVFTEwMe/bswd/fnzvu\nuIOIiAgAPDw8WLhwoTEHvOU887i4ONLT04mKinLEqQnRLTqcx94TZB67uFatWrWK6upqfvvb3zq6\nFCFa6bZ57EL0FTExMSxbtowlS5Zw7NgxYzHT+vXriY+P/8HvazKZjN7oQvQGGYoR4rL09HQGDhyI\np6cnQLPxdyGuJ3LFLkQbjh49SkBAQKvnt23bRkREBCdOnKCqqorZs2cTGhpKaGgoeXl5AJw8eZKo\nqCjMZjMLFixo1Q5XiJ4mwS5EJ6Wnp/PKK6+wfft2PD09WbJkCU899RT5+fmkpaWxYMECAF566SUm\nTpzIoUOHiImJoaKiwsGVi75GhmKE6IQ9e/ZQUFDArl27cHd3ByA7O5vi4mLjmDNnznD27Flyc3PZ\nunUrAD/5yU8YMmSIQ2oWfZcEuxCdMGrUKL788ktKS0uxWhsmJdTX17N3715cXV0dXJ0QzclQjBCd\nMHLkSNLS0nj00UeNFgRRUVH8+c9/No5p7DMzadIkNm3aBMCOHTs4depU7xcs+jQJdiE6ydfXl+Tk\nZGJjYykrK2P16tUUFBQQFBSEv78/a9asAeDFF18kNzcXs9nM1q1bZWMZ0etkgZIQQlwnZIGSEEL0\nURLsQgjhZCTYhRDCyUiwCyGEk5FgF0IIJyPBLoQQTkaCXQghnIwEuxBCOBkJdiGEcDIS7EII4WQk\n2IUQwslIsAshhJORYBdCCCcjwS6EEE5Ggl0IIZyMBLsQQjgZCXYhhHAyEuxCCOFkJNiFEMLJdCnY\nlVJ/UEp9oZQqVEqlK6U8uqswIYQQP0xXr9g/AAK01kFAKfCbrpckhBCiK7oU7FrrXVpr++WHe4Hb\nu16SEEKIrujOMfb5wI72XlRKPaGUKlBKFVRVVXXjlxVCCNHUDR0doJTKBrzaeGmF1jrz8jErADuQ\n3N77aK3fBt4GsFqt+gdVK4QQokMdBrvWetqVXldKzQPuA6ZqrSWwhRDCwToM9itRSs0AlgGTtda1\n3VOSEEKIrujqGPsbwCDgA6WUTSm1phtqEkII0QVdumLXWvt0VyFCCCG6h6w8FUIIJyPBLoQQTkaC\nXQghnIwEuxBCOBkJdiGEcDIS7EII4WQk2IUQwslIsAshhJORYBdCCCcjwS6EEE5Ggl0IIZyMckSn\nXaVUFfC/PfxlPIETPfw1rlVy7n2TnLvzG6m1HtbRQQ4J9t6glCrQWlsdXYcjyLnLufc1ffnc2yJD\nMUII4WQk2IUQwsk4c7C/7egCHEjOvW+ScxeAE4+xCyFEX+XMV+xCCNEnOXWwK6X+oJT6QilVqJRK\nV0p5OLqm3qKUilVKHVJK1Sul+sRsAaXUDKVUiVLqiFJquaPr6S1KqXeUUt8ppYocXUtvU0qNUErl\nKKWKL///vsTRNV0LnDrYgQ+AAK11EFAK/MbB9fSmImAWkOvoQnqDUqo/8CYQDfgDDyml/B1bVa9Z\nD8xwdBEOYgee0Vr7A+HA4j70794upw52rfUurbX98sO9wO2OrKc3aa0/11qXOLqOXjQeOKK1/lJr\nfRF4D7jfwTX1Cq11LvC9o+twBK11pdb6wOX/rgE+B25zbFWO59TB3sJ8YIejixA95jbgqyaPv0Z+\nwPsUpZQJuAv4xLGVON4Nji6gq5RS2YBXGy+t0FpnXj5mBQ1/siX3Zm09rTPnLkRfoJQaCKQBT2qt\nzzi6Hke77oNdaz3tSq8rpeYB9wFTtZPN7ezo3PuYb4ARTR7ffvk54eSUUi40hHqy1nqro+u5Fjj1\nUIxSagawDJipta51dD2iR+UDo5VS3kqpG4EHgfcdXJPoYUopBawDPtdav+boeq4VTh3swBvAIOAD\npZRNKbXG0QX1FqVUjFLqayAC2KaU2unomnrS5Zvk8cBOGm6g/Y/W+pBjq+odSqnNwMfAWKXU10qp\nxx1dUy+aADwCTLn8M25TSv2no4tyNFl5KoQQTsbZr9iFEKLPkWAXQggnI8EuhBBORoJdCCGcjAS7\nEEI4GQl2IYRwMhLsQgjhZCTYhRDCyfwfAYfW8HMTSgoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129d8ba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "embedding_matrix = classifier.get_variable_value('dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights')\n",
    "\n",
    "for term_index in range(len(informative_terms)):\n",
    "    # Create a one-hot encoding for our term. It has 0s everywhere, except for\n",
    "    # a single 1 in the coordinate that corresponds to that term.\n",
    "    term_vector = np.zeros(len(informative_terms))\n",
    "    term_vector[term_index] = 1\n",
    "    # We'll now project that one-hot vector into the embedding space.\n",
    "    embedding_xy = np.matmul(term_vector, embedding_matrix)\n",
    "    plt.text(embedding_xy[0],\n",
    "           embedding_xy[1],\n",
    "           informative_terms[term_index])\n",
    "\n",
    "# Do a little setup to make sure the plot displays nicely.\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 15)\n",
    "plt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6:  Try to improve the model's performance\n",
    "\n",
    "See if you can refine the model to improve performance. A couple things you may want to try:\n",
    "\n",
    "* **Changing hyperparameters**, or **using a different optimizer** like Adam (you may only gain one or two accuracy percentage points following these strategies).\n",
    "* **Adding additional terms to `informative_terms`.** There's a full vocabulary file with all 30,716 terms for this data set that you can use at: https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/terms.txt You can pick out additional terms from this vocabulary file, or use the whole thing via the `categorical_column_with_vocabulary_file` feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/terms.txt\n",
      "253952/253538 [==============================] - 1s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the vocabulary file.\n",
    "terms_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/terms.txt'\n",
    "terms_path = tf.keras.utils.get_file(terms_url.split('/')[-1], terms_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set metrics:\n",
      "accuracy 0.77248\n",
      "accuracy_baseline 0.5\n",
      "auc 0.8474759\n",
      "auc_precision_recall 0.84290093\n",
      "average_loss 0.48632672\n",
      "label/mean 0.5\n",
      "loss 12.158168\n",
      "precision 0.771481\n",
      "prediction/mean 0.50158817\n",
      "recall 0.77432\n",
      "global_step 1000\n",
      "---\n",
      "Test set metrics:\n",
      "accuracy 0.76188\n",
      "accuracy_baseline 0.5\n",
      "auc 0.84112865\n",
      "auc_precision_recall 0.8348567\n",
      "average_loss 0.49450433\n",
      "label/mean 0.5\n",
      "loss 12.362608\n",
      "precision 0.76223665\n",
      "prediction/mean 0.5015396\n",
      "recall 0.7612\n",
      "global_step 1000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Create a feature column from \"terms\", using a full vocabulary file.\n",
    "informative_terms = None\n",
    "with io.open(terms_path, 'r', encoding='utf8') as f:\n",
    "  # Convert it to a set first to remove duplicates.\n",
    "  informative_terms = list(set(f.read().split()))\n",
    "  \n",
    "terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=\"terms\", \n",
    "                                                                                 vocabulary_list=informative_terms)\n",
    "\n",
    "terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)\n",
    "feature_columns = [ terms_embedding_column ]\n",
    "\n",
    "my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=[10,10],\n",
    "  optimizer=my_optimizer\n",
    ")\n",
    "\n",
    "classifier.train(\n",
    "  input_fn=lambda: _input_fn([train_path]),\n",
    "  steps=1000)\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn([train_path]),\n",
    "  steps=1000)\n",
    "print(\"Training set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "  print(m, evaluation_metrics[m])\n",
    "print(\"---\")\n",
    "\n",
    "evaluation_metrics = classifier.evaluate(\n",
    "  input_fn=lambda: _input_fn([test_path]),\n",
    "  steps=1000)\n",
    "\n",
    "print(\"Test set metrics:\")\n",
    "for m in evaluation_metrics:\n",
    "  print(m, evaluation_metrics[m])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Final Word\n",
    "\n",
    "We may have gotten a DNN solution with an embedding that was better than our original linear model, but the linear model was also pretty good and was quite a bit faster to train. Linear models train more quickly because they do not have nearly as many parameters to update or layers to backprop through.\n",
    "\n",
    "In some applications, the speed of linear models may be a game changer, or linear models may be perfectly sufficient from a quality standpoint. In other areas, the additional model complexity and capacity provided by DNNs might be more important. When defining your model architecture, remember to explore your problem sufficiently so that you know which space you're in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
